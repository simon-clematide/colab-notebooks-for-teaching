{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-clematide/colab-notebooks-for-teaching/blob/main/OpenAI_Embeddings_Visualization_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link to this notebook"
      ],
      "metadata": {
        "id": "CZSf7idkS2iZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsH4Yg-Kfxkn"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Visualize Text Embeddings"
      ],
      "metadata": {
        "id": "aPVjQ04vf3wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"Hi, how are you?\"\n",
        "sentence2 = \"Hello, how's it going?\"\n",
        "\n",
        "sentence3 = \"I wonder what the weather will be like tomorrow?\""
      ],
      "metadata": {
        "id": "_BdcmB7_f1rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expectation:\n",
        "* `sentence1` and `sentence2` should be **close** in embedding space, since they are similar in meaning.\n",
        "* `sentence3` should be **farther apart**."
      ],
      "metadata": {
        "id": "T354GIoTf8bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the OpenAI API to get text embeddings\n",
        "\n",
        "from openai import OpenAI\n",
        "# we use colab userdata secrets to avoid accidental leaking of API KEYS\n",
        "from google.colab import userdata\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "sentence1_embedding = get_embedding(sentence1)\n",
        "sentence2_embedding = get_embedding(sentence2)\n",
        "sentence3_embedding = get_embedding(sentence3)"
      ],
      "metadata": {
        "id": "usZs3Zxtf6VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that we got a vector of numbers for each sentence\n",
        "sentence1_embedding[:10]"
      ],
      "metadata": {
        "id": "BFYAzKnsf-TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's **visualize** the embeddings of the three sentences in 2D space. \\\n",
        "\n",
        "There is no need to understand the function below in depth – it applies Multidimensional Scaling (MDS) to the embeddings.\\\n",
        "Embeddings with a smaller cosine distance will appear closer to each other in the plot."
      ],
      "metadata": {
        "id": "LoN_z3GNgZAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "\n",
        "def visualize_embeddings(sentences, embeddings):\n",
        "    assert len(sentences) == len(embeddings)\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    # Step 1: Compute the cosine similarity matrix\n",
        "    cosine_sim_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    # Step 2: Convert cosine similarities to distances (1 - similarity)\n",
        "    distance_matrix = 1 - cosine_sim_matrix\n",
        "\n",
        "    # Step 3: Use MDS to project the points into 2D based on the distance matrix\n",
        "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42, normalized_stress=\"auto\")\n",
        "    vis_dims = mds.fit_transform(distance_matrix)\n",
        "\n",
        "    # Step 4: Visualize the 2D projection\n",
        "    x = [x for x, y in vis_dims]\n",
        "    y = [y for x, y in vis_dims]\n",
        "\n",
        "    plt.scatter(x, y, c=list(range(len(embeddings))), alpha=0.6)\n",
        "\n",
        "    # Label each point with the corresponding sentence\n",
        "    for i, txt in enumerate(sentences):\n",
        "        plt.annotate(txt, (x[i], y[i]), fontsize=12)\n",
        "\n",
        "    # Step 5: Annotate the cosine distances between points\n",
        "    for i in range(len(embeddings)):\n",
        "        for j in range(i + 1, len(embeddings)):\n",
        "            # Retrieve cosine distance between the embeddings (from the original distance matrix)\n",
        "            dist = distance_matrix[i, j]\n",
        "\n",
        "            # Find the midpoint between the points to place the label\n",
        "            mid_x = (x[i] + x[j]) / 2\n",
        "            mid_y = (y[i] + y[j]) / 2\n",
        "\n",
        "            # Annotate the cosine distance on the plot at the midpoint\n",
        "            plt.text(mid_x, mid_y, f\"{dist:.2f}\", fontsize=10, color=\"blue\", ha=\"center\")\n",
        "\n",
        "            # Add an arrow from point i to point j\n",
        "            plt.arrow(x[i], y[i], x[j] - x[i], y[j] - y[i], color=\"gray\", alpha=0.1)\n",
        "\n",
        "    plt.gca().axes.get_xaxis().set_visible(False)\n",
        "    plt.gca().axes.get_yaxis().set_visible(False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ccN3LjW6gXIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's visualize the embeddings of the three sentences."
      ],
      "metadata": {
        "id": "tU6N-bOygeO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_embeddings(\n",
        "    sentences=[sentence1, sentence2, sentence3],\n",
        "    embeddings=[sentence1_embedding, sentence2_embedding, sentence3_embedding]\n",
        ")"
      ],
      "metadata": {
        "id": "DgQ7iwAAgao1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Use Text Embeddings for Spotting the Correct Translation"
      ],
      "metadata": {
        "id": "oV40sqoIgiZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do you say **\"Hi, how are you?\"** in Esperanto?\n",
        "\n",
        "a) \"Mi ne komprenas.\" \\\n",
        "b) \"Kiom longe for estas la Ejfel-turo?\" \\\n",
        "c) \"Saluton, kiel vi fartas?\"\n",
        "\n",
        "Find the correct translation simply using text embeddings."
      ],
      "metadata": {
        "id": "ZFot_IHoglS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "w5IgLCezg0DU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}