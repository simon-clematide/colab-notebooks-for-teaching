{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPs4zwFUG38BkVjI3DmDdHh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-clematide/colab-notebooks-for-teaching/blob/main/MGENRE_impresso_linking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Named Entity Linking (NEL) Model Exploration\n",
        "\n",
        "This notebook demonstrates the use of a multilingual Named Entity Linking (NEL) model\n",
        "from the 'impresso-project/nel-hipe-multilingual' collection. The main functionality\n",
        "focuses on linking entities from text to their corresponding Wikipedia entries, with\n",
        "confidence scores generated for each link.\n",
        "\n",
        "We'll use the Hugging Face `transformers` library to:\n",
        "- Load the model and tokenizer.\n",
        "- Run the model on a single sentence.\n",
        "- Extract and link entities from the input text to Wikipedia.\n",
        "\n",
        "Key functions:\n",
        "1. `load_model_and_tokenizer`: Loads the NEL model and tokenizer.\n",
        "2. `link_entity_to_wikipedia`: Extracts entities from a sentence and links them to Wikipedia entries.\n",
        "3. `run_single_sentence`: A streamlined function to run the model on a single sentence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gmSLj6JGsata"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Step 1: Load Model and Tokenizer\n",
        "def load_model_and_tokenizer(model_name=\"impresso-project/nel-hipe-multilingual\"):\n",
        "    \"\"\"\n",
        "    Loads the NEL model and tokenizer from Hugging Face.\n",
        "\n",
        "    Args:\n",
        "    model_name (str): The name of the pre-trained model to load.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the tokenizer and the model.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "# Step 2: Extract Entities\n",
        "def extract_entity(sentence):\n",
        "    \"\"\"\n",
        "    Extract the entity enclosed between [START] and [END] in the sentence.\n",
        "\n",
        "    Args:\n",
        "    sentence (str): The sentence containing the entity to extract.\n",
        "\n",
        "    Returns:\n",
        "    str: The extracted entity.\n",
        "    \"\"\"\n",
        "    start_token = \"[START]\"\n",
        "    end_token = \"[END]\"\n",
        "    start_idx = sentence.find(start_token) + len(start_token)\n",
        "    end_idx = sentence.find(end_token)\n",
        "    entity = sentence[start_idx:end_idx].strip()\n",
        "    return entity\n",
        "\n",
        "# Step 3: Link Entity to Wikipedia\n",
        "def link_entity_to_wikipedia(sentence, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Links the extracted entity to potential Wikipedia articles using the model.\n",
        "\n",
        "    Args:\n",
        "    sentence (str): The input sentence containing the entity.\n",
        "    tokenizer: The tokenizer used for processing the input.\n",
        "    model: The pre-trained NEL model for generating Wikipedia links.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the entity and possible Wikipedia links with confidence scores.\n",
        "    \"\"\"\n",
        "    # Extract the entity\n",
        "    entity = extract_entity(sentence)\n",
        "\n",
        "    # Process the sentence with the model\n",
        "    inputs = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        num_beams=5,\n",
        "        num_return_sequences=5,\n",
        "        output_scores=True,\n",
        "        return_dict_in_generate=True\n",
        "    )\n",
        "\n",
        "    # Decode the model outputs (representing possible Wikipedia links)\n",
        "    decoded_outputs = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
        "    logits_per_step = outputs.scores\n",
        "\n",
        "    # Calculate probabilities and format results\n",
        "    results = []\n",
        "    for seq_idx, decoded_seq in enumerate(decoded_outputs):\n",
        "        sequence_logits = logits_per_step[seq_idx]\n",
        "        probs_per_token = torch.softmax(sequence_logits, dim=-1).max(dim=-1)[0]\n",
        "        avg_confidence = probs_per_token.mean().item()\n",
        "        normalized_confidence = avg_confidence * 100\n",
        "\n",
        "        results.append({\n",
        "            \"wikipedia_link\": decoded_seq,\n",
        "            \"confidence\": f\"{normalized_confidence:.2f}%\"\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"entity\": entity,\n",
        "        \"links\": results\n",
        "    }\n",
        "\n",
        "# Step 4: Run the Model on a Single Sentence\n",
        "def run_single_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Encapsulates the functionality to run the NEL model on a single sentence.\n",
        "\n",
        "    Args:\n",
        "    sentence (str): The input sentence with the entity to be linked.\n",
        "\n",
        "    Returns:\n",
        "    dict: Results containing the extracted entity and possible Wikipedia links with confidence scores.\n",
        "    \"\"\"\n",
        "    # Load the model and tokenizer\n",
        "    tokenizer, model = load_model_and_tokenizer()\n",
        "\n",
        "    # Get Wikipedia link predictions\n",
        "    result = link_entity_to_wikipedia(sentence, tokenizer, model)\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"Original Sentence: {sentence}\")\n",
        "    print(f\"Entity: {result['entity']}\")\n",
        "    print(f\"Possible Wikipedia Links:\")\n",
        "    for link_info in result['links']:\n",
        "        print(f\" - {link_info['wikipedia_link']} (Confidence: {link_info['confidence']})\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "18MXfNmKsjmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the model on a sentence"
      ],
      "metadata": {
        "id": "o0buuzYwtKxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Running the model on a single sentence\n",
        "sentence = \"[START] United Press [END] - On the home front, the British populace remains steadfast in the face of ongoing air raids.\"\n",
        "run_single_sentence(sentence)"
      ],
      "metadata": {
        "id": "dD0hVc0sskQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vX4tgI79s5JX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}