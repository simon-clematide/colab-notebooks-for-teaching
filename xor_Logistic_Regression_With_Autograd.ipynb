{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-clematide/colab-notebooks-for-teaching/blob/main/xor_Logistic_Regression_With_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W64ubk-T3Yx"
      },
      "source": [
        "# XOR by Logistic Regression with autograd\n",
        "- Autograd was one of the first implementations of autodifferentiation in Python\n",
        "- Example adapted from https://github.com/HIPS/autograd\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPFQAVpJT3Yx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11ae024-41a4-4d74-a628-04bfa33f638b"
      },
      "source": [
        "import autograd.numpy as np\n",
        "from autograd import grad\n",
        "\n",
        "# monkey patch for nicer output https://github.com/HIPS/autograd/issues/355\n",
        "np.numpy_boxes.ArrayBox.__str__ = lambda self: str(self._value)\n",
        "\n",
        "# avoid numerical environment pollution ;-)\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 0.5*(np.tanh(x) + 1)\n",
        "\n",
        "def logistic_predictions(weights, inputs):\n",
        "    # Outputs probability of a label being true according to logistic model.\n",
        "    return sigmoid(np.dot(inputs, weights))\n",
        "\n",
        "def training_loss(weights):\n",
        "    # Training loss is the negative log-likelihood of the training labels.\n",
        "    # updates the predictions and label probabilities as globals to reflect the model changes\n",
        "    global preds, label_probabilities\n",
        "    preds = logistic_predictions(weights, inputs)\n",
        "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
        "    return -np.sum(np.log(label_probabilities))\n",
        "\n",
        "# Build a toy dataset.\n",
        "# x1, x2, bias\n",
        "inputs = np.array([[0.,0.,1.],\n",
        "                   [0.,1.,1.],\n",
        "                   [1.,0.,1.],\n",
        "                   [1.,1.,1.]])\n",
        "targets = np.array([False, True, True, False])\n",
        "\n",
        "# Build a function that returns gradients of training loss using autograd.\n",
        "training_gradient_fun = grad(training_loss)\n",
        "\n",
        "# initialize the weights randomly\n",
        "weights = np.random.rand(3)\n",
        "# or by zeroes\n",
        "# weights = np.array([0.,0.,0.])\n",
        "\n",
        "# learning rate\n",
        "lrate = 0.2\n",
        "\n",
        "# Optimize weights using gradient descent.\n",
        "print(f\"Initial loss:        {training_loss(weights):.4f}\")\n",
        "print(f\"Initial weights:     {weights}\")\n",
        "print(f\"Raw predictions:     {preds}\")\n",
        "print(f\"Label probabilities: {label_probabilities}\")\n",
        "print(f\"Targets:             {targets.astype(float)}\")\n",
        "print()\n",
        "for i in range(10):\n",
        "    print(f\"\\nIteration {i}:\")\n",
        "    print(f\"Current weights:     {weights}\")\n",
        "    gradients = training_gradient_fun(weights)\n",
        "    print(f\"Gradients:           {gradients}\")\n",
        "    weights -= lrate * gradients\n",
        "    print(f\"New weights:         {weights}\")\n",
        "    print(f\"Trained loss:        {training_loss(weights):.4f}\")\n",
        "    print(f\"Raw predictions:     {preds}\")\n",
        "    print(f\"Label probabilities: {label_probabilities}\")\n",
        "    print(f\"Targets:             {targets.astype(float)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss:        3.0605\n",
            "Initial weights:     [0.167 0.314 0.103]\n",
            "Raw predictions:     [0.551 0.697 0.632 0.763]\n",
            "Label probabilities: [0.449 0.697 0.632 0.237]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "\n",
            "Iteration 0:\n",
            "Current weights:     [0.167 0.314 0.103]\n",
            "Gradients:           [0.79  0.92  1.288]\n",
            "New weights:         [ 0.009  0.13  -0.154]\n",
            "Trained loss:        2.7953\n",
            "Raw predictions:     [0.423 0.488 0.428 0.492]\n",
            "Label probabilities: [0.577 0.488 0.428 0.508]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 1:\n",
            "Current weights:     [ 0.009  0.13  -0.154]\n",
            "Gradients:           [-0.159 -0.04  -0.337]\n",
            "New weights:         [ 0.041  0.138 -0.087]\n",
            "Trained loss:        2.7829\n",
            "Raw predictions:     [0.457 0.525 0.477 0.546]\n",
            "Label probabilities: [0.543 0.525 0.477 0.454]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 2:\n",
            "Current weights:     [ 0.041  0.138 -0.087]\n",
            "Gradients:           [0.046 0.142 0.01 ]\n",
            "New weights:         [ 0.032  0.109 -0.089]\n",
            "Trained loss:        2.7797\n",
            "Raw predictions:     [0.456 0.51  0.472 0.526]\n",
            "Label probabilities: [0.544 0.51  0.472 0.474]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 3:\n",
            "Current weights:     [ 0.032  0.109 -0.089]\n",
            "Gradients:           [-0.005  0.072 -0.073]\n",
            "New weights:         [ 0.033  0.095 -0.074]\n",
            "Trained loss:        2.7778\n",
            "Raw predictions:     [0.463 0.51  0.479 0.527]\n",
            "Label probabilities: [0.537 0.51  0.479 0.473]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 4:\n",
            "Current weights:     [ 0.033  0.095 -0.074]\n",
            "Gradients:           [ 0.012  0.074 -0.042]\n",
            "New weights:         [ 0.031  0.08  -0.066]\n",
            "Trained loss:        2.7765\n",
            "Raw predictions:     [0.467 0.507 0.482 0.522]\n",
            "Label probabilities: [0.533 0.507 0.482 0.478]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 5:\n",
            "Current weights:     [ 0.031  0.08  -0.066]\n",
            "Gradients:           [ 0.009  0.059 -0.043]\n",
            "New weights:         [ 0.029  0.068 -0.057]\n",
            "Trained loss:        2.7755\n",
            "Raw predictions:     [0.471 0.505 0.486 0.52 ]\n",
            "Label probabilities: [0.529 0.505 0.486 0.48 ]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 6:\n",
            "Current weights:     [ 0.029  0.068 -0.057]\n",
            "Gradients:           [ 0.011  0.05  -0.036]\n",
            "New weights:         [ 0.027  0.058 -0.05 ]\n",
            "Trained loss:        2.7748\n",
            "Raw predictions:     [0.475 0.504 0.488 0.517]\n",
            "Label probabilities: [0.525 0.504 0.488 0.483]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 7:\n",
            "Current weights:     [ 0.027  0.058 -0.05 ]\n",
            "Gradients:           [ 0.011  0.042 -0.032]\n",
            "New weights:         [ 0.024  0.05  -0.044]\n",
            "Trained loss:        2.7742\n",
            "Raw predictions:     [0.478 0.503 0.49  0.515]\n",
            "Label probabilities: [0.522 0.503 0.49  0.485]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 8:\n",
            "Current weights:     [ 0.024  0.05  -0.044]\n",
            "Gradients:           [ 0.011  0.036 -0.028]\n",
            "New weights:         [ 0.022  0.043 -0.038]\n",
            "Trained loss:        2.7738\n",
            "Raw predictions:     [0.481 0.502 0.492 0.513]\n",
            "Label probabilities: [0.519 0.502 0.492 0.487]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 9:\n",
            "Current weights:     [ 0.022  0.043 -0.038]\n",
            "Gradients:           [ 0.01   0.03  -0.024]\n",
            "New weights:         [ 0.02   0.036 -0.034]\n",
            "Trained loss:        2.7735\n",
            "Raw predictions:     [0.483 0.501 0.493 0.512]\n",
            "Label probabilities: [0.517 0.501 0.493 0.488]\n",
            "Targets:             [0. 1. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o69gxFqDT3Y1"
      },
      "source": [
        "## Feature engineering\n",
        "Let's construct an additional feature by hand: x1 * x2\n",
        "\n",
        "Can we now learn the xor function?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUO5mz4fT3Y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10e2fdf-6144-4486-8c47-09c86f0f41ff"
      },
      "source": [
        "# Build a toy dataset.\n",
        "# x1, x2, bias, x1*x2\n",
        "inputs = np.array([[0.,0.,1.,0.],\n",
        "                   [0.,1.,1.,0.],\n",
        "                   [1.,0.,1.,0.],\n",
        "                   [1.,1.,1.,1.]])\n",
        "targets = np.array([False, True, True, False])\n",
        "\n",
        "# initialize the weights\n",
        "weights = np.array([0.,0.,0.,0])\n",
        "\n",
        "# Optimize weights using gradient descent.\n",
        "print(f\"Initial loss:        {training_loss(weights):.4f}\")\n",
        "print(f\"Initial weights:     {weights}\")\n",
        "print(f\"Raw predictions:     {preds}\")\n",
        "print(f\"Label probabilities: {label_probabilities}\")\n",
        "\n",
        "print()\n",
        "for i in range(10):\n",
        "    print(f\"\\nIteration {i}:\")\n",
        "    print(f\"Current weights:     {weights}\")\n",
        "    gradients = training_gradient_fun(weights)\n",
        "    print(f\"Gradients:           {gradients}\")\n",
        "    weights -= gradients * 0.1\n",
        "    print(f\"New weights:         {weights}\")\n",
        "    print(f\"Trained loss:        {training_loss(weights):.4f}\")\n",
        "    print(f\"Raw predictions:     {preds}\")\n",
        "    print(f\"Label probabilities: {label_probabilities}\")\n",
        "    print(f\"Targets:             {targets.astype(float)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss:        2.7726\n",
            "Initial weights:     [0. 0. 0. 0.]\n",
            "Raw predictions:     [0.5 0.5 0.5 0.5]\n",
            "Label probabilities: [0.5 0.5 0.5 0.5]\n",
            "\n",
            "\n",
            "Iteration 0:\n",
            "Current weights:     [0. 0. 0. 0.]\n",
            "Gradients:           [0. 0. 0. 1.]\n",
            "New weights:         [ 0.   0.   0.  -0.1]\n",
            "Trained loss:        2.6776\n",
            "Raw predictions:     [0.5  0.5  0.5  0.45]\n",
            "Label probabilities: [0.5  0.5  0.5  0.55]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 1:\n",
            "Current weights:     [ 0.   0.   0.  -0.1]\n",
            "Gradients:           [-0.1 -0.1 -0.1  0.9]\n",
            "New weights:         [ 0.01  0.01  0.01 -0.19]\n",
            "Trained loss:        2.5958\n",
            "Raw predictions:     [0.505 0.51  0.51  0.421]\n",
            "Label probabilities: [0.495 0.51  0.51  0.579]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 2:\n",
            "Current weights:     [ 0.01  0.01  0.01 -0.19]\n",
            "Gradients:           [-0.139 -0.139 -0.109  0.841]\n",
            "New weights:         [ 0.024  0.024  0.021 -0.274]\n",
            "Trained loss:        2.5216\n",
            "Raw predictions:     [0.51  0.522 0.522 0.399]\n",
            "Label probabilities: [0.49  0.522 0.522 0.601]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 3:\n",
            "Current weights:     [ 0.024  0.024  0.021 -0.274]\n",
            "Gradients:           [-0.158 -0.158 -0.093  0.797]\n",
            "New weights:         [ 0.04   0.04   0.03  -0.354]\n",
            "Trained loss:        2.4536\n",
            "Raw predictions:     [0.515 0.535 0.535 0.38 ]\n",
            "Label probabilities: [0.485 0.535 0.535 0.62 ]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 4:\n",
            "Current weights:     [ 0.04   0.04   0.03  -0.354]\n",
            "Gradients:           [-0.17 -0.17 -0.07  0.76]\n",
            "New weights:         [ 0.057  0.057  0.037 -0.43 ]\n",
            "Trained loss:        2.3907\n",
            "Raw predictions:     [0.519 0.547 0.547 0.364]\n",
            "Label probabilities: [0.481 0.547 0.547 0.636]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 5:\n",
            "Current weights:     [ 0.057  0.057  0.037 -0.43 ]\n",
            "Gradients:           [-0.179 -0.179 -0.048  0.728]\n",
            "New weights:         [ 0.075  0.075  0.042 -0.503]\n",
            "Trained loss:        2.3321\n",
            "Raw predictions:     [0.521 0.558 0.558 0.349]\n",
            "Label probabilities: [0.479 0.558 0.558 0.651]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 6:\n",
            "Current weights:     [ 0.075  0.075  0.042 -0.503]\n",
            "Gradients:           [-0.186 -0.186 -0.028  0.698]\n",
            "New weights:         [ 0.093  0.093  0.045 -0.572]\n",
            "Trained loss:        2.2773\n",
            "Raw predictions:     [0.522 0.569 0.569 0.336]\n",
            "Label probabilities: [0.478 0.569 0.569 0.664]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 7:\n",
            "Current weights:     [ 0.093  0.093  0.045 -0.572]\n",
            "Gradients:           [-0.192 -0.192 -0.01   0.671]\n",
            "New weights:         [ 0.112  0.112  0.046 -0.64 ]\n",
            "Trained loss:        2.2256\n",
            "Raw predictions:     [0.523 0.578 0.578 0.323]\n",
            "Label probabilities: [0.477 0.578 0.578 0.677]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 8:\n",
            "Current weights:     [ 0.112  0.112  0.046 -0.64 ]\n",
            "Gradients:           [-0.197 -0.197  0.006  0.647]\n",
            "New weights:         [ 0.132  0.132  0.045 -0.704]\n",
            "Trained loss:        2.1767\n",
            "Raw predictions:     [0.523 0.588 0.588 0.312]\n",
            "Label probabilities: [0.477 0.588 0.588 0.688]\n",
            "Targets:             [0. 1. 1. 0.]\n",
            "\n",
            "Iteration 9:\n",
            "Current weights:     [ 0.132  0.132  0.045 -0.704]\n",
            "Gradients:           [-0.201 -0.201  0.02   0.624]\n",
            "New weights:         [ 0.152  0.152  0.043 -0.767]\n",
            "Trained loss:        2.1302\n",
            "Raw predictions:     [0.522 0.596 0.596 0.302]\n",
            "Label probabilities: [0.478 0.596 0.596 0.698]\n",
            "Targets:             [0. 1. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVhPYELUT3Y3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}