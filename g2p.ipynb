{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8oM2zBWgnFBLRJf5yAyky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-clematide/colab-notebooks-for-teaching/blob/main/g2p.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# g2p with Neural Transducer\n",
        "This notebook allows the training and evaluation of  g2p models for any language. It uses the sigmorphon tsv-separated input format."
      ],
      "metadata": {
        "id": "BGBkcPhkZje6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "The most important step is to use a `T4 GPU` for efficientt batch training with larger datasets and larger models!\n",
        "Please select now Runtime>Change Runtime Setting!\n",
        "\n",
        "After that install the neural transducer package. If you change the runtime afterwards all local data will be lost."
      ],
      "metadata": {
        "id": "pVHPp9M5ZgrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install  git+https://github.com/simon-clematide/il-reimplementation.git@development-for-colab#egg=neural-transducer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l61ywRyqU1RG",
        "outputId": "9b01143d-e14f-45a6-b0dd-5a0955075b63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neural-transducer\n",
            "  Cloning https://github.com/simon-clematide/il-reimplementation.git (to revision development-for-colab) to /tmp/pip-install-xk4yl6f0/neural-transducer_94c36a2a9e6f44189023fd701dcb694b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/simon-clematide/il-reimplementation.git /tmp/pip-install-xk4yl6f0/neural-transducer_94c36a2a9e6f44189023fd701dcb694b\n",
            "  Running command git checkout -b development-for-colab --track origin/development-for-colab\n",
            "  Switched to a new branch 'development-for-colab'\n",
            "  Branch 'development-for-colab' set up to track remote branch 'development-for-colab' from 'origin'.\n",
            "  Resolved https://github.com/simon-clematide/il-reimplementation.git to commit 67f4ba213bfbc4c607100d67edb2d9893ea02977\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from neural-transducer) (3.0.5)\n",
            "Building wheels for collected packages: neural-transducer\n",
            "  Building wheel for neural-transducer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neural-transducer: filename=neural_transducer-0.2.1-py3-none-any.whl size=35819 sha256=dc885e19a5023190b7a0b42988807bfac4ba328082fdcde947e54d56f01ca2b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p71yjcd3/wheels/59/80/3e/173d614ac8c06576dad633620a49b2cffea451e93403ec0c5b\n",
            "Successfully built neural-transducer\n",
            "Installing collected packages: neural-transducer\n",
            "Successfully installed neural-transducer-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This installs the main command line commands for training, evaluation and prediction: `trans-train`\n",
        "\n",
        "See the commandline help for more information."
      ],
      "metadata": {
        "id": "1e8eR9rggyzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! trans-train -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWQpQ8SyhHhi",
        "outputId": "6fcd1f3d-eef0-4a64-a151-c7ac34c156c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: trans-train [-h] [--pytorch-seed PYTORCH_SEED] [--train TRAIN]\n",
            "                   [--precomputed-train PRECOMPUTED_TRAIN] [--save-precomputed-train]\n",
            "                   [--vocabulary VOCABULARY] --dev DEV [--test TEST] --output OUTPUT [--nfd]\n",
            "                   [--char-dim CHAR_DIM] [--feat-dim FEAT_DIM] [--action-dim ACTION_DIM]\n",
            "                   [--enc-type {lstm,transformer}] [--dec-hidden-dim DEC_HIDDEN_DIM]\n",
            "                   [--dec-layers DEC_LAYERS] [--beam-width BEAM_WIDTH] [--patience PATIENCE]\n",
            "                   [--epochs EPOCHS] [--batch-size BATCH_SIZE] [--eval-batch-size EVAL_BATCH_SIZE]\n",
            "                   [--loss-reduction {sum,mean}] [--grad-accumulation GRAD_ACCUMULATION]\n",
            "                   [--train-subset-eval-size TRAIN_SUBSET_EVAL_SIZE]\n",
            "                   [--optimizer {adam,adamw,adadelta}] [--scheduler {inv_sr,reduce_on_plateau}]\n",
            "                   [--sed-em-iterations SED_EM_ITERATIONS] [--sed-params SED_PARAMS]\n",
            "                   [--device DEVICE]\n",
            "\n",
            "Train a g2p neural transducer.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --pytorch-seed PYTORCH_SEED\n",
            "                        Random seed used by PyTorch.\n",
            "  --train TRAIN         Path to train set data. Only required if --precomputed-train and\n",
            "                        --vocabulary is notprovided.\n",
            "  --precomputed-train PRECOMPUTED_TRAIN\n",
            "                        Path to precomputed train set data. If provided, --vocabulary option must\n",
            "                        be provided, as well.\n",
            "  --save-precomputed-train\n",
            "                        Store the precomputed training set (i.e., containing the expert's\n",
            "                        information neededfor training). Can be used to speed up the training\n",
            "                        process for large datasets.\n",
            "  --vocabulary VOCABULARY\n",
            "                        Path to the vocabulary. If provided, --precomputed-train must be provided,\n",
            "                        as well.\n",
            "  --dev DEV             Path to development set data.\n",
            "  --test TEST           Path to development set data.\n",
            "  --output OUTPUT       Output directory.\n",
            "  --nfd                 Train on NFD-normalized data. Write out in NFC.\n",
            "  --char-dim CHAR_DIM   Character peak_embedding dimension.\n",
            "  --feat-dim FEAT_DIM   Feature embedding dimension, if any.The data is assumed to be in UniMorph\n",
            "                        format.\n",
            "  --action-dim ACTION_DIM\n",
            "                        Action peak_embedding dimension.\n",
            "  --enc-type {lstm,transformer}\n",
            "                        Type of used encoder.\n",
            "  --dec-hidden-dim DEC_HIDDEN_DIM\n",
            "                        Decoder LSTM state dimension.\n",
            "  --dec-layers DEC_LAYERS\n",
            "                        Number of decoder LSTM layers.\n",
            "  --beam-width BEAM_WIDTH\n",
            "                        Beam width for beam search decoding. A value < 1 will disable beam search\n",
            "                        decoding.\n",
            "  --patience PATIENCE   Maximal patience for early stopping.\n",
            "  --epochs EPOCHS       Maximal number of training epochs.\n",
            "  --batch-size BATCH_SIZE\n",
            "                        Batch size for training.\n",
            "  --eval-batch-size EVAL_BATCH_SIZE\n",
            "                        Batch size for evaluation. Will be set to training batch size (--batch-\n",
            "                        size) if notspecified.\n",
            "  --loss-reduction {sum,mean}\n",
            "                        How the loss is reduced during training.\n",
            "  --grad-accumulation GRAD_ACCUMULATION\n",
            "                        Gradient accumulation.\n",
            "  --train-subset-eval-size TRAIN_SUBSET_EVAL_SIZE\n",
            "                        Percentage of training data used to evaluate training accuracy every epoch\n",
            "                        (randomly sampled).\n",
            "  --optimizer {adam,adamw,adadelta}\n",
            "                        Optimizer used in training.\n",
            "  --scheduler {inv_sr,reduce_on_plateau}\n",
            "                        Scheduler used in training.\n",
            "  --sed-em-iterations SED_EM_ITERATIONS\n",
            "                        SED EM iterations.\n",
            "  --sed-params SED_PARAMS\n",
            "                        Path to learned SED parameters.\n",
            "  --device DEVICE       Device to run training on.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /usr/local/lib/python3.10/dist-packages/trans/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fekixv7zWteb",
        "outputId": "fa9600bd-fc14-4035-effd-6a7164576fca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actions.py     grid_search.py\t  optimal_expert_substitutions.py  sed.py\t  utils.py\n",
            "encoders.py    __init__.py\t  optimizers.py\t\t\t   train.py\t  vocabulary.py\n",
            "ensembling.py  optimal_expert.py  __pycache__\t\t\t   transducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load sigmorphon data sets\n",
        "The sigmorphon 2021 contains low-resource training setup (800 training items per language), medium (8000 training items), and one buggy high-resource (33344 training items)."
      ],
      "metadata": {
        "id": "HsbgN-YvmMKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone --depth 1 https://github.com/sigmorphon/2021-task1.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ujqhtuioz6v",
        "outputId": "32035c93-e023-434c-e307-0414ddc81462"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '2021-task1'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 97 (delta 2), reused 71 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (97/97), 1.28 MiB | 10.36 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ln -s 2021-task1/data sigmorphon2021\n",
        "! wc -l sigmorphon2021/*/*.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rj2QH26mz2k",
        "outputId": "3167d6e5-eea7-4e4a-daf3-e9fa04953bf5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   4168 sigmorphon2021/high/eng_us_dev.tsv\n",
            "   4168 sigmorphon2021/high/eng_us_test.tsv\n",
            "  33344 sigmorphon2021/high/eng_us_train.tsv\n",
            "    100 sigmorphon2021/low/ady_dev.tsv\n",
            "    100 sigmorphon2021/low/ady_test.tsv\n",
            "    800 sigmorphon2021/low/ady_train.tsv\n",
            "    100 sigmorphon2021/low/gre_dev.tsv\n",
            "    100 sigmorphon2021/low/gre_test.tsv\n",
            "    800 sigmorphon2021/low/gre_train.tsv\n",
            "    100 sigmorphon2021/low/ice_dev.tsv\n",
            "    100 sigmorphon2021/low/ice_test.tsv\n",
            "    800 sigmorphon2021/low/ice_train.tsv\n",
            "    100 sigmorphon2021/low/ita_dev.tsv\n",
            "    100 sigmorphon2021/low/ita_test.tsv\n",
            "    800 sigmorphon2021/low/ita_train.tsv\n",
            "    100 sigmorphon2021/low/khm_dev.tsv\n",
            "    100 sigmorphon2021/low/khm_test.tsv\n",
            "    800 sigmorphon2021/low/khm_train.tsv\n",
            "    100 sigmorphon2021/low/lav_dev.tsv\n",
            "    100 sigmorphon2021/low/lav_test.tsv\n",
            "    800 sigmorphon2021/low/lav_train.tsv\n",
            "    100 sigmorphon2021/low/mlt_latn_dev.tsv\n",
            "    100 sigmorphon2021/low/mlt_latn_test.tsv\n",
            "    800 sigmorphon2021/low/mlt_latn_train.tsv\n",
            "    100 sigmorphon2021/low/rum_dev.tsv\n",
            "    100 sigmorphon2021/low/rum_test.tsv\n",
            "    800 sigmorphon2021/low/rum_train.tsv\n",
            "    100 sigmorphon2021/low/slv_dev.tsv\n",
            "    100 sigmorphon2021/low/slv_test.tsv\n",
            "    800 sigmorphon2021/low/slv_train.tsv\n",
            "    100 sigmorphon2021/low/wel_sw_dev.tsv\n",
            "    100 sigmorphon2021/low/wel_sw_test.tsv\n",
            "    800 sigmorphon2021/low/wel_sw_train.tsv\n",
            "   1000 sigmorphon2021/medium/arm_e_dev.tsv\n",
            "   1000 sigmorphon2021/medium/arm_e_test.tsv\n",
            "   8000 sigmorphon2021/medium/arm_e_train.tsv\n",
            "   1000 sigmorphon2021/medium/bul_dev.tsv\n",
            "   1000 sigmorphon2021/medium/bul_test.tsv\n",
            "   8000 sigmorphon2021/medium/bul_train.tsv\n",
            "   1000 sigmorphon2021/medium/dut_dev.tsv\n",
            "   1000 sigmorphon2021/medium/dut_test.tsv\n",
            "   8000 sigmorphon2021/medium/dut_train.tsv\n",
            "   1000 sigmorphon2021/medium/fre_dev.tsv\n",
            "   1000 sigmorphon2021/medium/fre_test.tsv\n",
            "   8000 sigmorphon2021/medium/fre_train.tsv\n",
            "   1000 sigmorphon2021/medium/geo_dev.tsv\n",
            "   1000 sigmorphon2021/medium/geo_test.tsv\n",
            "   8000 sigmorphon2021/medium/geo_train.tsv\n",
            "   1000 sigmorphon2021/medium/hbs_latn_dev.tsv\n",
            "   1000 sigmorphon2021/medium/hbs_latn_test.tsv\n",
            "   8000 sigmorphon2021/medium/hbs_latn_train.tsv\n",
            "   1000 sigmorphon2021/medium/hun_dev.tsv\n",
            "   1000 sigmorphon2021/medium/hun_test.tsv\n",
            "   8000 sigmorphon2021/medium/hun_train.tsv\n",
            "   1000 sigmorphon2021/medium/jpn_hira_dev.tsv\n",
            "   1000 sigmorphon2021/medium/jpn_hira_test.tsv\n",
            "   8000 sigmorphon2021/medium/jpn_hira_train.tsv\n",
            "   1000 sigmorphon2021/medium/kor_dev.tsv\n",
            "   1000 sigmorphon2021/medium/kor_test.tsv\n",
            "   8000 sigmorphon2021/medium/kor_train.tsv\n",
            "   1000 sigmorphon2021/medium/vie_hanoi_dev.tsv\n",
            "   1000 sigmorphon2021/medium/vie_hanoi_test.tsv\n",
            "   8000 sigmorphon2021/medium/vie_hanoi_train.tsv\n",
            " 151680 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmorphon 2020 contains 3600 training items per language."
      ],
      "metadata": {
        "id": "IKSI06YRoLu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone --depth 1 https://github.com/sigmorphon/2020.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX7aQ31_VeF9",
        "outputId": "f5e6056b-06fb-4a5c-8c7a-978a5843fc2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '2020'...\n",
            "remote: Enumerating objects: 258, done.\u001b[K\n",
            "remote: Counting objects: 100% (258/258), done.\u001b[K\n",
            "remote: Compressing objects: 100% (206/206), done.\u001b[K\n",
            "remote: Total 258 (delta 56), reused 227 (delta 52), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (258/258), 15.71 MiB | 13.40 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ln -fs 2020/task1/data sigmorphon2020\n",
        "! wc -l sigmorphon2020/*/*.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXE80xhWWH-H",
        "outputId": "e3927d79-80e6-4df9-d712-4a015aed810e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    450 sigmorphon2020/dev/ady_dev.tsv\n",
            "    450 sigmorphon2020/dev/arm_dev.tsv\n",
            "    450 sigmorphon2020/dev/bul_dev.tsv\n",
            "    450 sigmorphon2020/dev/dut_dev.tsv\n",
            "    450 sigmorphon2020/dev/fre_dev.tsv\n",
            "    450 sigmorphon2020/dev/geo_dev.tsv\n",
            "    450 sigmorphon2020/dev/gre_dev.tsv\n",
            "    450 sigmorphon2020/dev/hin_dev.tsv\n",
            "    450 sigmorphon2020/dev/hun_dev.tsv\n",
            "    450 sigmorphon2020/dev/ice_dev.tsv\n",
            "    450 sigmorphon2020/dev/jpn_dev.tsv\n",
            "    450 sigmorphon2020/dev/kor_dev.tsv\n",
            "    450 sigmorphon2020/dev/lit_dev.tsv\n",
            "    450 sigmorphon2020/dev/rum_dev.tsv\n",
            "    450 sigmorphon2020/dev/vie_dev.tsv\n",
            "    450 sigmorphon2020/test/ady_test.tsv\n",
            "    450 sigmorphon2020/test/arm_test.tsv\n",
            "    450 sigmorphon2020/test/bul_test.tsv\n",
            "    450 sigmorphon2020/test/dut_test.tsv\n",
            "    450 sigmorphon2020/test/fre_test.tsv\n",
            "    450 sigmorphon2020/test/geo_test.tsv\n",
            "    450 sigmorphon2020/test/gre_test.tsv\n",
            "    450 sigmorphon2020/test/hin_test.tsv\n",
            "    450 sigmorphon2020/test/hun_test.tsv\n",
            "    450 sigmorphon2020/test/ice_test.tsv\n",
            "    450 sigmorphon2020/test/jpn_test.tsv\n",
            "    450 sigmorphon2020/test/kor_test.tsv\n",
            "    450 sigmorphon2020/test/lit_test.tsv\n",
            "    450 sigmorphon2020/test/rum_test.tsv\n",
            "    450 sigmorphon2020/test/vie_test.tsv\n",
            "   3600 sigmorphon2020/train/ady_train.tsv\n",
            "   3600 sigmorphon2020/train/arm_train.tsv\n",
            "   3600 sigmorphon2020/train/bul_train.tsv\n",
            "   3600 sigmorphon2020/train/dut_train.tsv\n",
            "   3600 sigmorphon2020/train/fre_train.tsv\n",
            "   3600 sigmorphon2020/train/geo_train.tsv\n",
            "   3600 sigmorphon2020/train/gre_train.tsv\n",
            "   3600 sigmorphon2020/train/hin_train.tsv\n",
            "   3600 sigmorphon2020/train/hun_train.tsv\n",
            "   3600 sigmorphon2020/train/ice_train.tsv\n",
            "   3600 sigmorphon2020/train/jpn_train.tsv\n",
            "   3600 sigmorphon2020/train/kor_train.tsv\n",
            "   3600 sigmorphon2020/train/lit_train.tsv\n",
            "   3600 sigmorphon2020/train/rum_train.tsv\n",
            "   3600 sigmorphon2020/train/vie_train.tsv\n",
            "  67500 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Command line options explained\n",
        "### Data:\n",
        "- **`--train TRAIN`**: Required path to the training dataset: `path/to/LANG_trains.tsv`. Required unless `--precomputed-train` is used along with `--vocabulary`.\n",
        "- **`--dev DEV`**: Required path to the development (validation) dataset: `path/to/LANG_dev.tsv`\n",
        "- **`--test TEST`**: Optional path to the test dataset: `path/to/LANG_test.tsv`\n",
        "- **`--output OUTPUT`**: The output directory where the training results will be saved: `path/to/directory`. If output directory exists, the trainer stops.\n",
        "- **`--precomputed-train PRECOMPUTED_TRAIN`**: Path to the precomputed training dataset. Must be accompanied by `--vocabulary`.\n",
        "- **`--save-precomputed-train`**: If flag is set, stores the precomputed training dataset actions. Useful for speeding up subsequent training runs on large datasets. Create file `precomputed_train.pkl` in output directory.\n",
        "- **`--vocabulary VOCABULARY`**: Path to the vocabulary file: `path/to/vocabulary.pkl` Required if `--precomputed-train` is provided.\n",
        "\n",
        "\n",
        "\n",
        "### Model Configuration:\n",
        "- **`--char-dim CHAR_DIM`**: The dimensionality of the character embedding vector. Defaults to 100.\n",
        "- **`--feat-dim FEAT_DIM`**: The dimensionality of any additional feature embeddings, assuming UniMorph format data. Defaults to\n",
        "- **`--action-dim ACTION_DIM`**: The dimensionality of the action embeddings.\n",
        "- **`--enc-type {lstm,transformer}`**: The type of encoder to use, either LSTM or Transformer. Defaults to `lstm`.\n",
        "- **`--enc-layers ENC_LAYERS`**: The number of layers in the encoder. Defaults 1.\n",
        "- **`--enc-dropout ENC_DROPOUT`**: The internal layer dropout. Defaults 0.\n",
        "- **`--dec-hidden-dim DEC_HIDDEN_DIM`**: The dimension of the hidden state in the LSTM decoder. Defaults to 200.\n",
        "- **`--dec-layers DEC_LAYERS`**: The number of layers in the LSTM decoder. Defaults 1.\n",
        "\n",
        "### Training and Evaluation:\n",
        "- **`--pytorch-seed PYTORCH_SEED`**: Sets the random seed for PyTorch to ensure reproducibility of results. Random number is not set.\n",
        "- **`--epochs EPOCHS`**: The maximum number of epochs to train. Defaults to 60.\n",
        "- **`--batch-size BATCH_SIZE`**: The batch size for training. Defaults to 5.\n",
        "- **`--eval-batch-size EVAL_BATCH_SIZE`**: The batch size for evaluation. Defaults to the same size as `--batch-size` if not specified.\n",
        "- **`--patience PATIENCE`**: The number of epochs to wait for improvement before early stopping. Defaults to 12.\n",
        "- **`--train-subset-eval-size TRAIN_SUBSET_EVAL_SIZE`**: The proportion of training data to use for evaluating training accuracy each epoch. Defaults to 5 (5% of the data)\n",
        "\n",
        "### Optimization:\n",
        "- **`--optimizer {adam,adamw,adadelta}`**: The optimizer to use during training. Defaults to `adadelta`.\n",
        "- **`--scheduler {inv_sr,reduce_on_plateau}`**: The scheduler to use for adjusting the learning rate. Defaults to\n",
        "- **`--loss-reduction {sum,mean}`**: The method for reducing the loss during training, either by summing or averaging. Defaults to `mean`.\n",
        "- **`--grad-accumulation GRAD_ACCUMULATION`**: The number of steps over which gradients are accumulated before performing an optimization step.\n",
        "\n",
        "### Decoding and Regularization:\n",
        "- **`--beam-width BEAM_WIDTH`**: The width of the beam for beam search decoding. A value less than 1 disables beam search. Defaults to 4.\n",
        "- **`--nfd`**: If flag is set, trains on NFD-normalized data and outputs results in NFC. Helps for highly composed script systems like Hangul.\n",
        "\n",
        "### Device Configuration:\n",
        "- **`--device DEVICE`**: Specifies the device to run the training on (e.g., CPU, GPU). Defaults to `cpu`. Set to `cuda` for GPU\n",
        "\n",
        "### Specialized Training Options:\n",
        "- **`--sed-em-iterations SED_EM_ITERATIONS`**: The number of Expectation-Maximization iterations for SED. Defaults to 10. Can be time-consuming.\n",
        "- **`--sed-params SED_PARAMS`**: Path to learned SED parameters. `path/to/sed.pkl`\n"
      ],
      "metadata": {
        "id": "8cehJTqqKDCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a low-resource model"
      ],
      "metadata": {
        "id": "WkfsH6yepZRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing a good SED model happens on CPU and takes some time. Therefore it's better to compute them once per data set and reuse it later. We use dummy training parameters to avoid long training."
      ],
      "metadata": {
        "id": "0RlpbhQussv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p sed-2021/low_ita/\n",
        "! trans-train --train sigmorphon2021/low/ita_train.tsv --dev sigmorphon2021/low/ita_dev.tsv --sed-em-iterations 5 --output /tmp/sed-ita-2021/low_ita \\\n",
        "      --epochs 1 --beam-width 0 --batch-size 200 --device cuda \\\n",
        "  && cp /tmp/sed-ita-2021/low_ita/sed.pkl sed-2021/low_ita && rm -r /tmp/sed-ita-2021/low_ita"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2a6JFdAs22Z",
        "outputId": "000ef5a2-ebaa-4477-e284-b3e659e3e1d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: pytorch_seed   : None\n",
            "INFO: train          : sigmorphon2021/low/ita_train.tsv\n",
            "INFO: precomputed_train: None\n",
            "INFO: save_precomputed_train: False\n",
            "INFO: vocabulary     : None\n",
            "INFO: dev            : sigmorphon2021/low/ita_dev.tsv\n",
            "INFO: test           : None\n",
            "INFO: output         : /tmp/sed-ita-2021/low_ita\n",
            "INFO: nfd            : False\n",
            "INFO: char_dim       : 100\n",
            "INFO: feat_dim       : None\n",
            "INFO: action_dim     : 100\n",
            "INFO: enc_type       : lstm\n",
            "INFO: dec_hidden_dim : 200\n",
            "INFO: dec_layers     : 1\n",
            "INFO: beam_width     : 0\n",
            "INFO: patience       : 12\n",
            "INFO: epochs         : 1\n",
            "INFO: batch_size     : 200\n",
            "INFO: eval_batch_size: None\n",
            "INFO: loss_reduction : mean\n",
            "INFO: grad_accumulation: 1\n",
            "INFO: train_subset_eval_size: 5\n",
            "INFO: optimizer      : adadelta\n",
            "INFO: scheduler      : None\n",
            "INFO: sed_em_iterations: 5\n",
            "INFO: sed_params     : None\n",
            "INFO: device         : cuda\n",
            "INFO: enc_hidden_dim : 200\n",
            "INFO: enc_layers     : 1\n",
            "INFO: enc_bidirectional: True\n",
            "INFO: enc_dropout    : 0.0\n",
            "INFO: lr             : 1.0\n",
            "INFO: rho            : 0.9\n",
            "INFO: opt_eps        : 1e-06\n",
            "INFO: weight_decay   : 0.0\n",
            "INFO: Will perform training on unnormalized data.\n",
            "INFO: 65 actions: Vocabulary({⟪: 0, ⟫: 1, '<PAD>': 2, ConditionalDel(): 3, ConditionalCopy(): 4, ConditionalSub(new='a'): 5, ConditionalIns(new='a'): 6, ConditionalSub(new=' '): 7, ConditionalIns(new=' '): 8, ConditionalSub(new='b'): 9, ConditionalIns(new='b'): 10, ConditionalSub(new='n'): 11, ConditionalIns(new='n'): 12, ConditionalSub(new='d'): 13, ConditionalIns(new='d'): 14, ConditionalSub(new='o'): 15, ConditionalIns(new='o'): 16, ConditionalSub(new='t'): 17, ConditionalIns(new='t'): 18, ConditionalSub(new='j'): 19, ConditionalIns(new='j'): 20, ConditionalSub(new='m'): 21, ConditionalIns(new='m'): 22, ConditionalSub(new='e'): 23, ConditionalIns(new='e'): 24, ConditionalSub(new='k'): 25, ConditionalIns(new='k'): 26, ConditionalSub(new='ʎ'): 27, ConditionalIns(new='ʎ'): 28, ConditionalSub(new='ɛ'): 29, ConditionalIns(new='ɛ'): 30, ConditionalSub(new='͡'): 31, ConditionalIns(new='͡'): 32, ConditionalSub(new='s'): 33, ConditionalIns(new='s'): 34, ConditionalSub(new='p'): 35, ConditionalIns(new='p'): 36, ConditionalSub(new='ɲ'): 37, ConditionalIns(new='ɲ'): 38, ConditionalSub(new='r'): 39, ConditionalIns(new='r'): 40, ConditionalSub(new='ɔ'): 41, ConditionalIns(new='ɔ'): 42, ConditionalSub(new='w'): 43, ConditionalIns(new='w'): 44, ConditionalSub(new='ʒ'): 45, ConditionalIns(new='ʒ'): 46, ConditionalSub(new='i'): 47, ConditionalIns(new='i'): 48, ConditionalSub(new='l'): 49, ConditionalIns(new='l'): 50, ConditionalSub(new='ɡ'): 51, ConditionalIns(new='ɡ'): 52, ConditionalSub(new='ʃ'): 53, ConditionalIns(new='ʃ'): 54, ConditionalSub(new='u'): 55, ConditionalIns(new='u'): 56, ConditionalSub(new='v'): 57, ConditionalIns(new='v'): 58, ConditionalSub(new='̯'): 59, ConditionalIns(new='̯'): 60, ConditionalSub(new='z'): 61, ConditionalIns(new='z'): 62, ConditionalSub(new='f'): 63, ConditionalIns(new='f'): 64})\n",
            "INFO: 33 chars: Vocabulary({⟪: 0, ⟫: 1, '<PAD>': 2, '<UNK>': 3, 'a': 4, 'b': 5, 'n': 6, 'd': 7, 'o': 8, 't': 9, 'i': 10, 'm': 11, 'e': 12, 'c': 13, 'g': 14, 'l': 15, 'z': 16, 'p': 17, 'r': 18, 'q': 19, 'u': 20, 's': 21, 'h': 22, 'v': 23, 'à': 24, 'f': 25, 'ò': 26, 'ù': 27, 'ì': 28, 'é': 29, 'y': 30, 'k': 31, 'w': 32})\n",
            "INFO: Wrote vocabulary to /tmp/sed-ita-2021/low_ita/vocabulary.pkl.\n",
            "INFO: Updating model parameters by maximizing likelihood using EM (5 iterations).\n",
            "INFO: Initial weighted LL=-85.7199\n",
            "INFO: IT_0=-44.1698\n",
            "INFO: IT_1=-37.1937\n",
            "INFO: IT_2=-35.1240\n",
            "INFO: IT_3=-34.9291\n",
            "INFO: IT_4=-34.8779\n",
            "INFO: Wrote latest model weights to /tmp/sed-ita-2021/low_ita/sed.pkl.\n",
            "INFO: Precomputing optimal actions for training samples.\n",
            "|                                                                                   | ETA:  --:--:--INFO: Training for a maximum of 1 with a maximum patience of 12.\n",
            "INFO: Number of train batches: 4.\n",
            "INFO: Training...\n",
            "INFO: \t\t...4 batches\n",
            "INFO: \t...finished in 1.348 sec.\n",
            "INFO: Average train loss: 3.4013.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...1 batches\n",
            "INFO: \t...finished in 0.273 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...1 batches\n",
            "INFO: \t...finished in 0.247 sec.\n",
            "INFO: Epoch 0 / 0: train loss: 3.4013 dev loss: 0.0127 train acc: 0.0000 dev acc: 0.0000 best train acc: 0.0000 best dev acc: 0.0000 best epoch: 0 patience: 1 / 11\n",
            "|                                                                                   | ETA:  --:--:--INFO: Finished training.\n",
            "rm: cannot remove '/tmp/sed-ita-2021/low_ita': Is a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf result_ita_low_sed3_defaults  # remove earlier runs\n",
        "! trans-train --train sigmorphon2021/low/ita_train.tsv --dev sigmorphon2021/low/ita_dev.tsv  --test sigmorphon2021/low/ita_test.tsv \\\n",
        "  --output result_ita_low_sed3_defaults \\\n",
        "  --enc-layers 2 --enc-dropout 0.1 \\\n",
        "  --batch-size 20 --beam-width 0 --epochs 100 --patience 20 --scheduler reduce_on_plateau --optimizer adam \\\n",
        "  --sed-params sed-2021/low_ita/sed.pkl --device cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRn7NXf-XDEG",
        "outputId": "c236fe35-dff4-46db-854e-75f884b9ed3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: pytorch_seed   : None\n",
            "INFO: train          : sigmorphon2021/low/ita_train.tsv\n",
            "INFO: precomputed_train: None\n",
            "INFO: save_precomputed_train: False\n",
            "INFO: vocabulary     : None\n",
            "INFO: dev            : sigmorphon2021/low/ita_dev.tsv\n",
            "INFO: test           : sigmorphon2021/low/ita_test.tsv\n",
            "INFO: output         : result_ita_low_sed3_defaults\n",
            "INFO: nfd            : False\n",
            "INFO: char_dim       : 100\n",
            "INFO: feat_dim       : None\n",
            "INFO: action_dim     : 100\n",
            "INFO: enc_type       : lstm\n",
            "INFO: dec_hidden_dim : 200\n",
            "INFO: dec_layers     : 1\n",
            "INFO: beam_width     : 0\n",
            "INFO: patience       : 20\n",
            "INFO: epochs         : 100\n",
            "INFO: batch_size     : 20\n",
            "INFO: eval_batch_size: None\n",
            "INFO: loss_reduction : mean\n",
            "INFO: grad_accumulation: 1\n",
            "INFO: train_subset_eval_size: 5\n",
            "INFO: optimizer      : adam\n",
            "INFO: scheduler      : reduce_on_plateau\n",
            "INFO: sed_em_iterations: 10\n",
            "INFO: sed_params     : sed-2021/low_ita/sed.pkl\n",
            "INFO: device         : cuda\n",
            "INFO: enc_hidden_dim : 200\n",
            "INFO: enc_layers     : 2\n",
            "INFO: enc_bidirectional: True\n",
            "INFO: enc_dropout    : 0.1\n",
            "INFO: lr             : 0.001\n",
            "INFO: betas          : (0.9, 0.999)\n",
            "INFO: eps            : 1e-08\n",
            "INFO: weight_decay   : 0\n",
            "INFO: amsgrad        : False\n",
            "INFO: factor         : 0.1\n",
            "INFO: lrs_patience   : 10\n",
            "INFO: threshold      : 0.0001\n",
            "INFO: threshold_mode : rel\n",
            "INFO: cooldown       : 0\n",
            "INFO: min_lr         : 0.0\n",
            "INFO: lrs_eps        : 1e-08\n",
            "INFO: verbose        : False\n",
            "INFO: Will perform training on unnormalized data.\n",
            "INFO: 65 actions: Vocabulary({⟪: 0, ⟫: 1, '<PAD>': 2, ConditionalDel(): 3, ConditionalCopy(): 4, ConditionalSub(new='a'): 5, ConditionalIns(new='a'): 6, ConditionalSub(new=' '): 7, ConditionalIns(new=' '): 8, ConditionalSub(new='b'): 9, ConditionalIns(new='b'): 10, ConditionalSub(new='n'): 11, ConditionalIns(new='n'): 12, ConditionalSub(new='d'): 13, ConditionalIns(new='d'): 14, ConditionalSub(new='o'): 15, ConditionalIns(new='o'): 16, ConditionalSub(new='t'): 17, ConditionalIns(new='t'): 18, ConditionalSub(new='j'): 19, ConditionalIns(new='j'): 20, ConditionalSub(new='m'): 21, ConditionalIns(new='m'): 22, ConditionalSub(new='e'): 23, ConditionalIns(new='e'): 24, ConditionalSub(new='k'): 25, ConditionalIns(new='k'): 26, ConditionalSub(new='ʎ'): 27, ConditionalIns(new='ʎ'): 28, ConditionalSub(new='ɛ'): 29, ConditionalIns(new='ɛ'): 30, ConditionalSub(new='͡'): 31, ConditionalIns(new='͡'): 32, ConditionalSub(new='s'): 33, ConditionalIns(new='s'): 34, ConditionalSub(new='p'): 35, ConditionalIns(new='p'): 36, ConditionalSub(new='ɲ'): 37, ConditionalIns(new='ɲ'): 38, ConditionalSub(new='r'): 39, ConditionalIns(new='r'): 40, ConditionalSub(new='ɔ'): 41, ConditionalIns(new='ɔ'): 42, ConditionalSub(new='w'): 43, ConditionalIns(new='w'): 44, ConditionalSub(new='ʒ'): 45, ConditionalIns(new='ʒ'): 46, ConditionalSub(new='i'): 47, ConditionalIns(new='i'): 48, ConditionalSub(new='l'): 49, ConditionalIns(new='l'): 50, ConditionalSub(new='ɡ'): 51, ConditionalIns(new='ɡ'): 52, ConditionalSub(new='ʃ'): 53, ConditionalIns(new='ʃ'): 54, ConditionalSub(new='u'): 55, ConditionalIns(new='u'): 56, ConditionalSub(new='v'): 57, ConditionalIns(new='v'): 58, ConditionalSub(new='̯'): 59, ConditionalIns(new='̯'): 60, ConditionalSub(new='z'): 61, ConditionalIns(new='z'): 62, ConditionalSub(new='f'): 63, ConditionalIns(new='f'): 64})\n",
            "INFO: 33 chars: Vocabulary({⟪: 0, ⟫: 1, '<PAD>': 2, '<UNK>': 3, 'a': 4, 'b': 5, 'n': 6, 'd': 7, 'o': 8, 't': 9, 'i': 10, 'm': 11, 'e': 12, 'c': 13, 'g': 14, 'l': 15, 'z': 16, 'p': 17, 'r': 18, 'q': 19, 'u': 20, 's': 21, 'h': 22, 'v': 23, 'à': 24, 'f': 25, 'ò': 26, 'ù': 27, 'ì': 28, 'é': 29, 'y': 30, 'k': 31, 'w': 32})\n",
            "INFO: Wrote vocabulary to result_ita_low_sed3_defaults/vocabulary.pkl.\n",
            "INFO: Loading sed channel parameters from file: sed-2021/low_ita/sed.pkl\n",
            "INFO: Precomputing optimal actions for training samples.\n",
            "|                                                                                   | ETA:  --:--:--INFO: Training for a maximum of 100 with a maximum patience of 20.\n",
            "INFO: Number of train batches: 40.\n",
            "INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.634 sec.\n",
            "INFO: Average train loss: 1.4151.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.052 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.123 sec.\n",
            "INFO: Epoch 0 / 99: train loss: 1.4151 dev loss: 0.0230 train acc: 0.0000 dev acc: 0.0000 best train acc: 0.0000 best dev acc: 0.0000 best epoch: 0 patience: 1 / 19\n",
            "|                                                                                   | ETA:  --:--:--INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.311 sec.\n",
            "INFO: Average train loss: 0.6509.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.039 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.112 sec.\n",
            "INFO: Found best dev accuracy 0.2400.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 1 / 99: train loss: 0.6509 dev loss: 0.0055 train acc: 0.3750 dev acc: 0.2400 best train acc: 0.3750 best dev acc: 0.2400 best epoch: 1 patience: 0 / 19\n",
            "|                                                                                   | ETA:   0:03:48INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.317 sec.\n",
            "INFO: Average train loss: 0.5536.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.045 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.117 sec.\n",
            "INFO: Epoch 2 / 99: train loss: 0.5536 dev loss: 0.0048 train acc: 0.3750 dev acc: 0.2400 best train acc: 0.3750 best dev acc: 0.2400 best epoch: 1 patience: 1 / 19\n",
            "|>                                                                                  | ETA:   0:02:16INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.317 sec.\n",
            "INFO: Average train loss: 0.4780.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.039 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.112 sec.\n",
            "INFO: Epoch 3 / 99: train loss: 0.4780 dev loss: 0.0056 train acc: 0.3750 dev acc: 0.2400 best train acc: 0.3750 best dev acc: 0.2400 best epoch: 1 patience: 2 / 19\n",
            "|>>                                                                                 | ETA:   0:01:45INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.311 sec.\n",
            "INFO: Average train loss: 0.3855.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.038 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.105 sec.\n",
            "INFO: Found best dev accuracy 0.3600.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 4 / 99: train loss: 0.3855 dev loss: 0.0051 train acc: 0.4500 dev acc: 0.3600 best train acc: 0.4500 best dev acc: 0.3600 best epoch: 4 patience: 0 / 19\n",
            "|>>>                                                                                | ETA:   0:01:29INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.324 sec.\n",
            "INFO: Average train loss: 0.3215.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.039 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.107 sec.\n",
            "INFO: Epoch 5 / 99: train loss: 0.3215 dev loss: 0.0052 train acc: 0.4500 dev acc: 0.3600 best train acc: 0.4500 best dev acc: 0.3600 best epoch: 4 patience: 1 / 19\n",
            "|>>>>                                                                               | ETA:   0:01:19INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.322 sec.\n",
            "INFO: Average train loss: 0.2823.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.040 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.112 sec.\n",
            "INFO: Found best dev accuracy 0.3900.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 6 / 99: train loss: 0.2823 dev loss: 0.0047 train acc: 0.5000 dev acc: 0.3900 best train acc: 0.5000 best dev acc: 0.3900 best epoch: 6 patience: 0 / 19\n",
            "|>>>>                                                                               | ETA:   0:01:13INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.336 sec.\n",
            "INFO: Average train loss: 0.2496.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.043 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.114 sec.\n",
            "INFO: Found best dev accuracy 0.4200.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 7 / 99: train loss: 0.2496 dev loss: 0.0060 train acc: 0.5500 dev acc: 0.4200 best train acc: 0.5500 best dev acc: 0.4200 best epoch: 7 patience: 0 / 19\n",
            "|>>>>>                                                                              | ETA:   0:01:08INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.322 sec.\n",
            "INFO: Average train loss: 0.2194.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.042 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.114 sec.\n",
            "INFO: Epoch 8 / 99: train loss: 0.2194 dev loss: 0.0062 train acc: 0.5250 dev acc: 0.3900 best train acc: 0.5500 best dev acc: 0.4200 best epoch: 7 patience: 1 / 19\n",
            "|>>>>>>                                                                             | ETA:   0:01:05INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.351 sec.\n",
            "INFO: Average train loss: 0.1928.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.383 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.945 sec.\n",
            "INFO: Found best dev accuracy 0.4300.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 9 / 99: train loss: 0.1928 dev loss: 0.0079 train acc: 0.5750 dev acc: 0.4300 best train acc: 0.5750 best dev acc: 0.4300 best epoch: 9 patience: 0 / 19\n",
            "|>>>>>>>                                                                            | ETA:   0:01:14INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.427 sec.\n",
            "INFO: Average train loss: 0.1695.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.247 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.612 sec.\n",
            "INFO: Found best dev accuracy 0.5200.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 10 / 99: train loss: 0.1695 dev loss: 0.0056 train acc: 0.6250 dev acc: 0.5200 best train acc: 0.6250 best dev acc: 0.5200 best epoch: 10 patience: 0 / 19\n",
            "|>>>>>>>>                                                                           | ETA:   0:01:18INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.444 sec.\n",
            "INFO: Average train loss: 0.1469.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.232 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.289 sec.\n",
            "INFO: Found best dev accuracy 0.5300.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 11 / 99: train loss: 0.1469 dev loss: 0.0039 train acc: 0.6250 dev acc: 0.5300 best train acc: 0.6250 best dev acc: 0.5300 best epoch: 11 patience: 0 / 19\n",
            "|>>>>>>>>>                                                                          | ETA:   0:01:18INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.332 sec.\n",
            "INFO: Average train loss: 0.1287.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.041 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.109 sec.\n",
            "INFO: Found best dev accuracy 0.5900.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 12 / 99: train loss: 0.1287 dev loss: 0.0036 train acc: 0.7750 dev acc: 0.5900 best train acc: 0.7750 best dev acc: 0.5900 best epoch: 12 patience: 0 / 19\n",
            "|>>>>>>>>>                                                                          | ETA:   0:01:14INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.325 sec.\n",
            "INFO: Average train loss: 0.1110.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.042 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.112 sec.\n",
            "INFO: Found best dev accuracy 0.6300.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 13 / 99: train loss: 0.1110 dev loss: 0.0029 train acc: 0.8750 dev acc: 0.6300 best train acc: 0.8750 best dev acc: 0.6300 best epoch: 13 patience: 0 / 19\n",
            "|>>>>>>>>>>                                                                         | ETA:   0:01:11INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.318 sec.\n",
            "INFO: Average train loss: 0.0969.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.048 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.121 sec.\n",
            "INFO: Epoch 14 / 99: train loss: 0.0969 dev loss: 0.0028 train acc: 0.9000 dev acc: 0.6300 best train acc: 0.9000 best dev acc: 0.6300 best epoch: 13 patience: 1 / 19\n",
            "|>>>>>>>>>>>                                                                        | ETA:   0:01:08INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.324 sec.\n",
            "INFO: Average train loss: 0.0856.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.042 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.129 sec.\n",
            "INFO: Epoch 15 / 99: train loss: 0.0856 dev loss: 0.0025 train acc: 0.9000 dev acc: 0.5900 best train acc: 0.9000 best dev acc: 0.6300 best epoch: 13 patience: 2 / 19\n",
            "|>>>>>>>>>>>>                                                                       | ETA:   0:01:05INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.357 sec.\n",
            "INFO: Average train loss: 0.0729.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.066 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.119 sec.\n",
            "INFO: Epoch 16 / 99: train loss: 0.0729 dev loss: 0.0021 train acc: 0.9250 dev acc: 0.6200 best train acc: 0.9250 best dev acc: 0.6300 best epoch: 13 patience: 3 / 19\n",
            "|>>>>>>>>>>>>>                                                                      | ETA:   0:01:03INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.331 sec.\n",
            "INFO: Average train loss: 0.0664.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.041 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.113 sec.\n",
            "INFO: Epoch 17 / 99: train loss: 0.0664 dev loss: 0.0022 train acc: 0.9000 dev acc: 0.6000 best train acc: 0.9250 best dev acc: 0.6300 best epoch: 13 patience: 4 / 19\n",
            "|>>>>>>>>>>>>>>                                                                     | ETA:   0:01:01INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.311 sec.\n",
            "INFO: Average train loss: 0.0595.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.042 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.117 sec.\n",
            "INFO: Found best dev accuracy 0.6900.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 18 / 99: train loss: 0.0595 dev loss: 0.0018 train acc: 0.9250 dev acc: 0.6900 best train acc: 0.9250 best dev acc: 0.6900 best epoch: 18 patience: 0 / 19\n",
            "|>>>>>>>>>>>>>>                                                                     | ETA:   0:00:59INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.319 sec.\n",
            "INFO: Average train loss: 0.0510.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.043 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.111 sec.\n",
            "INFO: Epoch 19 / 99: train loss: 0.0510 dev loss: 0.0019 train acc: 0.9250 dev acc: 0.6700 best train acc: 0.9250 best dev acc: 0.6900 best epoch: 18 patience: 1 / 19\n",
            "|>>>>>>>>>>>>>>>                                                                    | ETA:   0:00:58INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.316 sec.\n",
            "INFO: Average train loss: 0.0486.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.040 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.121 sec.\n",
            "INFO: Epoch 20 / 99: train loss: 0.0486 dev loss: 0.0018 train acc: 0.9500 dev acc: 0.6500 best train acc: 0.9500 best dev acc: 0.6900 best epoch: 18 patience: 2 / 19\n",
            "|>>>>>>>>>>>>>>>>                                                                   | ETA:   0:00:56INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.322 sec.\n",
            "INFO: Average train loss: 0.0507.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.040 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.116 sec.\n",
            "INFO: Epoch 21 / 99: train loss: 0.0507 dev loss: 0.0016 train acc: 0.9250 dev acc: 0.6500 best train acc: 0.9500 best dev acc: 0.6900 best epoch: 18 patience: 3 / 19\n",
            "|>>>>>>>>>>>>>>>>>                                                                  | ETA:   0:00:54INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.318 sec.\n",
            "INFO: Average train loss: 0.0417.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.039 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.108 sec.\n",
            "INFO: Found best dev accuracy 0.7000.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 22 / 99: train loss: 0.0417 dev loss: 0.0014 train acc: 0.9250 dev acc: 0.7000 best train acc: 0.9500 best dev acc: 0.7000 best epoch: 22 patience: 0 / 19\n",
            "|>>>>>>>>>>>>>>>>>>                                                                 | ETA:   0:00:53INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.318 sec.\n",
            "INFO: Average train loss: 0.0336.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.040 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.112 sec.\n",
            "INFO: Epoch 23 / 99: train loss: 0.0336 dev loss: 0.0014 train acc: 0.9500 dev acc: 0.7000 best train acc: 0.9500 best dev acc: 0.7000 best epoch: 22 patience: 1 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>                                                                | ETA:   0:00:52INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.314 sec.\n",
            "INFO: Average train loss: 0.0302.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.038 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.108 sec.\n",
            "INFO: Found best dev accuracy 0.7100.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 24 / 99: train loss: 0.0302 dev loss: 0.0011 train acc: 0.9250 dev acc: 0.7100 best train acc: 0.9500 best dev acc: 0.7100 best epoch: 24 patience: 0 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>                                                                | ETA:   0:00:50INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.333 sec.\n",
            "INFO: Average train loss: 0.0282.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.044 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.115 sec.\n",
            "INFO: Epoch 25 / 99: train loss: 0.0282 dev loss: 0.0012 train acc: 0.9750 dev acc: 0.6900 best train acc: 0.9750 best dev acc: 0.7100 best epoch: 24 patience: 1 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>                                                               | ETA:   0:00:49INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.321 sec.\n",
            "INFO: Average train loss: 0.0265.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.043 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.109 sec.\n",
            "INFO: Found best dev accuracy 0.7300.\n",
            "INFO: Saved new best model to result_ita_low_sed3_defaults/best.model.\n",
            "INFO: Epoch 26 / 99: train loss: 0.0265 dev loss: 0.0010 train acc: 0.9500 dev acc: 0.7300 best train acc: 0.9750 best dev acc: 0.7300 best epoch: 26 patience: 0 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>                                                              | ETA:   0:00:48INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.331 sec.\n",
            "INFO: Average train loss: 0.0244.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.042 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.109 sec.\n",
            "INFO: Epoch 27 / 99: train loss: 0.0244 dev loss: 0.0010 train acc: 0.9750 dev acc: 0.7000 best train acc: 0.9750 best dev acc: 0.7300 best epoch: 26 patience: 1 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>                                                             | ETA:   0:00:47INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.313 sec.\n",
            "INFO: Average train loss: 0.0227.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.041 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.112 sec.\n",
            "INFO: Epoch 28 / 99: train loss: 0.0227 dev loss: 0.0010 train acc: 0.9500 dev acc: 0.6800 best train acc: 0.9750 best dev acc: 0.7300 best epoch: 26 patience: 2 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>                                                            | ETA:   0:00:46INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.330 sec.\n",
            "INFO: Average train loss: 0.0186.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.043 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.116 sec.\n",
            "INFO: Epoch 29 / 99: train loss: 0.0186 dev loss: 0.0009 train acc: 0.9750 dev acc: 0.6900 best train acc: 0.9750 best dev acc: 0.7300 best epoch: 26 patience: 3 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>                                                           | ETA:   0:00:45INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.321 sec.\n",
            "INFO: Average train loss: 0.0157.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.045 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.120 sec.\n",
            "INFO: Epoch 30 / 99: train loss: 0.0157 dev loss: 0.0008 train acc: 1.0000 dev acc: 0.7100 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 4 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>                                                           | ETA:   0:00:44INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.337 sec.\n",
            "INFO: Average train loss: 0.0139.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.045 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.112 sec.\n",
            "INFO: Epoch 31 / 99: train loss: 0.0139 dev loss: 0.0007 train acc: 1.0000 dev acc: 0.7000 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 5 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>                                                          | ETA:   0:00:43INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.387 sec.\n",
            "INFO: Average train loss: 0.0124.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.057 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.152 sec.\n",
            "INFO: Epoch 32 / 99: train loss: 0.0124 dev loss: 0.0007 train acc: 1.0000 dev acc: 0.7000 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 6 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>                                                         | ETA:   0:00:42INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.447 sec.\n",
            "INFO: Average train loss: 0.0112.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.057 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.152 sec.\n",
            "INFO: Epoch 33 / 99: train loss: 0.0112 dev loss: 0.0007 train acc: 1.0000 dev acc: 0.7200 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 7 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                        | ETA:   0:00:41INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.420 sec.\n",
            "INFO: Average train loss: 0.0107.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.052 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.158 sec.\n",
            "INFO: Epoch 34 / 99: train loss: 0.0107 dev loss: 0.0008 train acc: 1.0000 dev acc: 0.7000 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 8 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                       | ETA:   0:00:41INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.434 sec.\n",
            "INFO: Average train loss: 0.0091.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.056 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.149 sec.\n",
            "INFO: Epoch 35 / 99: train loss: 0.0091 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.7200 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 9 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                      | ETA:   0:00:40INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.466 sec.\n",
            "INFO: Average train loss: 0.0083.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.054 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.158 sec.\n",
            "INFO: Epoch 36 / 99: train loss: 0.0083 dev loss: 0.0007 train acc: 1.0000 dev acc: 0.7000 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 10 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                      | ETA:   0:00:40INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.458 sec.\n",
            "INFO: Average train loss: 0.0086.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.058 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.159 sec.\n",
            "INFO: Epoch 37 / 99: train loss: 0.0086 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.6700 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 11 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                     | ETA:   0:00:39INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.322 sec.\n",
            "INFO: Average train loss: 0.0075.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.039 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.108 sec.\n",
            "INFO: Epoch 38 / 99: train loss: 0.0075 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.6800 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 12 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                    | ETA:   0:00:38INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.310 sec.\n",
            "INFO: Average train loss: 0.0064.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.042 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.112 sec.\n",
            "INFO: Epoch 39 / 99: train loss: 0.0064 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.7100 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 13 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                   | ETA:   0:00:37INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.325 sec.\n",
            "INFO: Average train loss: 0.0060.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.045 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.108 sec.\n",
            "INFO: Epoch 40 / 99: train loss: 0.0060 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.7100 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 14 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                  | ETA:   0:00:37INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.312 sec.\n",
            "INFO: Average train loss: 0.0059.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.041 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.110 sec.\n",
            "INFO: Epoch 41 / 99: train loss: 0.0059 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.7200 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 15 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                 | ETA:   0:00:36INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.311 sec.\n",
            "INFO: Average train loss: 0.0058.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.042 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.109 sec.\n",
            "INFO: Epoch 42 / 99: train loss: 0.0058 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.7200 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 16 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                 | ETA:   0:00:35INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.308 sec.\n",
            "INFO: Average train loss: 0.0056.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.041 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.111 sec.\n",
            "INFO: Epoch 43 / 99: train loss: 0.0056 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.7200 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 17 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                | ETA:   0:00:34INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.322 sec.\n",
            "INFO: Average train loss: 0.0056.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.041 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.114 sec.\n",
            "INFO: Epoch 44 / 99: train loss: 0.0056 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.7200 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 18 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                               | ETA:   0:00:33INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.322 sec.\n",
            "INFO: Average train loss: 0.0055.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.041 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.113 sec.\n",
            "INFO: Epoch 45 / 99: train loss: 0.0055 dev loss: 0.0006 train acc: 1.0000 dev acc: 0.7300 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 19 / 19\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                              | ETA:   0:00:33INFO: Training...\n",
            "INFO: \t\t...40 batches\n",
            "INFO: \t...finished in 0.342 sec.\n",
            "INFO: Average train loss: 0.0054.\n",
            "INFO: Evaluating on training data subset...\n",
            "INFO: \t\t...2 batches\n",
            "INFO: \t...finished in 0.044 sec.\n",
            "INFO: Evaluating on development data...\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.117 sec.\n",
            "INFO: Epoch 46 / 99: train loss: 0.0054 dev loss: 0.0005 train acc: 1.0000 dev acc: 0.7200 best train acc: 1.0000 best dev acc: 0.7300 best epoch: 26 patience: 20 / 19\n",
            "INFO: Out of patience after 47 epochs.\n",
            "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>| Time:  0:00:27\n",
            "INFO: Finished training.\n",
            "INFO: Evaluating best model on dev data using greedy decoding\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.122 sec.\n",
            "INFO: Dev set accuracy: 0.7300.\n",
            "INFO: Evaluating best model on test data using greedy decoding\n",
            "INFO: \t\t...5 batches\n",
            "INFO: \t...finished in 0.117 sec.\n",
            "INFO: Test set accuracy: 0.7500.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the output of the training and evaluation"
      ],
      "metadata": {
        "id": "5uKTa6sUypSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -lh result_ita_low_sed3_defaults/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGPQaoWxYrnq",
        "outputId": "2f6ea2cb-0ceb-4845-eb23-36cd49a76abb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 7.8M\n",
            "-rw-r--r-- 1 root root 7.8M Nov 20 22:11 best.model\n",
            "-rw-r--r-- 1 root root  896 Nov 20 22:11 dev_greedy.eval\n",
            "-rw-r--r-- 1 root root 2.3K Nov 20 22:11 dev_greedy.predictions\n",
            "-rw-r--r-- 1 root root  897 Nov 20 22:11 test_greedy.eval\n",
            "-rw-r--r-- 1 root root 2.2K Nov 20 22:11 test_greedy.predictions\n",
            "-rw-r--r-- 1 root root  945 Nov 20 22:11 train.log\n",
            "-rw-r--r-- 1 root root 1.1K Nov 20 22:10 vocabulary.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare the ground truth with the output for easy consumption"
      ],
      "metadata": {
        "id": "vIrbUqSNpHNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! diff -y --suppress-common result_ita_low_sed3_defaults/test_greedy.predictions 2021-task1/data/low/ita_test.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P8VK7piYsZQ",
        "outputId": "a920d58a-e302-4435-9820-dbd739696477"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avesse\ta v ɛ s s e\t\t\t\t\t      |\tavesse\ta v e s s e\n",
            "biscotto\tb i s k o t t o\t\t\t\t      |\tbiscotto\tb i s k ɔ t t o\n",
            "cellula\tt͡ʃ e l l u l a\t\t\t\t\t      |\tcellula\tt͡ʃ ɛ l l u l a\n",
            "cuoco\tk u ɔ k o\t\t\t\t\t      |\tcuoco\tk w ɔ k o\n",
            "difficile\td i f f i t͡ʃ i l e\t\t\t      |\tdifficile\td i f f i t ʃ i l e\n",
            "dirompere\td i r ɔ m p e r e\t\t\t      |\tdirompere\td i r o m p e r e\n",
            "effetto\te f f e t t o\t\t\t\t\t      |\teffetto\te f f ɛ t t o\n",
            "episodio\te p i z o d j o\t\t\t\t      |\tepisodio\te p i z ɔ d j o\n",
            "linea\tl i n ɛ a\t\t\t\t\t      |\tlinea\tl i n e a\n",
            "maria\tm a r j a\t\t\t\t\t      |\tmaria\tm a r i a\n",
            "mio\tm j o\t\t\t\t\t\t      |\tmio\tm i o\n",
            "nome\tn ɔ m e\t\t\t\t\t\t      |\tnome\tn o m e\n",
            "ormai\tɔ r m a i\t\t\t\t\t      |\tormai\to r m a i\n",
            "ottenere\to t t ɛ n e r e\t\t\t\t      |\tottenere\to t t e n e r e\n",
            "ragazzo\tr a ɡ a t t t s t s o\t\t\t\t      |\tragazzo\tr a ɡ a t t͡s o\n",
            "rumeno\tr u m e n o\t\t\t\t\t      |\trumeno\tr u m ɛ n o\n",
            "so\ts o\t\t\t\t\t\t      |\tso\ts ɔ\n",
            "stirpe\ts t j r p e\t\t\t\t\t      |\tstirpe\ts t i r p e\n",
            "sì\ts ì\t\t\t\t\t\t      |\tsì\ts i\n",
            "tempestivo\tt e m p ɛ s t i v o\t\t\t      |\ttempestivo\tt e m p e s t i v o\n",
            "trasporto\tt r a s p o r t o\t\t\t      |\ttrasporto\tt r a s p ɔ r t o\n",
            "undici\tu n d i t͡ʃ i\t\t\t\t\t      |\tundici\tu n d i t ʃ i\n",
            "venezia\tv e n e t t͡s j a\t\t\t\t      |\tvenezia\tv e n ɛ t t͡s j a\n",
            "weekend\tw ɛ e j ɛ n d\t\t\t\t\t      |\tweekend\tw i k ɛ n d\n",
            "york\tk o r k\t\t\t\t\t\t      \\\tyork\tj ɔ r k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Go to http://ipa-reader.xyz/ for pronouncing IPA phones in different languages (English, Italian, French, etc.) White-space must be removed.\n",
        "\n"
      ],
      "metadata": {
        "id": "bLx9x5iy0Z9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the official evaluation script for the computation of WER and LER"
      ],
      "metadata": {
        "id": "0VXdgIp0G1bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! paste 2021-task1/data/low/ita_test.tsv result_ita_low_sed3_defaults/test_greedy.predictions | cut -f 2,4 > test.tsv\n",
        "! python 2020/task1/evaluation/evaluate.py test.tsv\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4M7LZSn1LcD",
        "outputId": "1d734177-7e44-4611-e9b5-f6f736170d18"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Incorrect prediction:\t't͡ʃ ɛ l l u l a' (predicted: 't͡ʃ e l l u l a')\n",
            "WARNING: Incorrect prediction:\t'k w ɔ k o' (predicted: 'k u ɔ k o')\n",
            "WARNING: Incorrect prediction:\t'd i f f i t ʃ i l e' (predicted: 'd i f f i t͡ʃ i l e')\n",
            "WARNING: Incorrect prediction:\t'a v e s s e' (predicted: 'a v ɛ s s e')\n",
            "WARNING: Incorrect prediction:\t'b i s k ɔ t t o' (predicted: 'b i s k o t t o')\n",
            "WARNING: Incorrect prediction:\t'd i r o m p e r e' (predicted: 'd i r ɔ m p e r e')\n",
            "WARNING: Incorrect prediction:\t'e f f ɛ t t o' (predicted: 'e f f e t t o')\n",
            "WARNING: Incorrect prediction:\t'l i n e a' (predicted: 'l i n ɛ a')\n",
            "WARNING: Incorrect prediction:\t'e p i z ɔ d j o' (predicted: 'e p i z o d j o')\n",
            "WARNING: Incorrect prediction:\t'm i o' (predicted: 'm j o')\n",
            "WARNING: Incorrect prediction:\t'm a r i a' (predicted: 'm a r j a')\n",
            "WARNING: Incorrect prediction:\t'r a ɡ a t t͡s o' (predicted: 'r a ɡ a t t t s t s o')\n",
            "WARNING: Incorrect prediction:\t'n o m e' (predicted: 'n ɔ m e')\n",
            "WARNING: Incorrect prediction:\t'o r m a i' (predicted: 'ɔ r m a i')\n",
            "WARNING: Incorrect prediction:\t'o t t e n e r e' (predicted: 'o t t ɛ n e r e')\n",
            "WARNING: Incorrect prediction:\t'r u m ɛ n o' (predicted: 'r u m e n o')\n",
            "WARNING: Incorrect prediction:\t's ɔ' (predicted: 's o')\n",
            "WARNING: Incorrect prediction:\t's t i r p e' (predicted: 's t j r p e')\n",
            "WARNING: Incorrect prediction:\t's i' (predicted: 's ì')\n",
            "WARNING: Incorrect prediction:\t't r a s p ɔ r t o' (predicted: 't r a s p o r t o')\n",
            "WARNING: Incorrect prediction:\t't e m p e s t i v o' (predicted: 't e m p ɛ s t i v o')\n",
            "WARNING: Incorrect prediction:\t'u n d i t ʃ i' (predicted: 'u n d i t͡ʃ i')\n",
            "WARNING: Incorrect prediction:\t'v e n ɛ t t͡s j a' (predicted: 'v e n e t t͡s j a')\n",
            "WARNING: Incorrect prediction:\t'w i k ɛ n d' (predicted: 'w ɛ e j ɛ n d')\n",
            "WARNING: Incorrect prediction:\t'j ɔ r k' (predicted: 'k o r k')\n",
            "WER:\t25.00\n",
            "LER:\t5.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YcSzG-LK0YTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7fVCsCW_I7V8"
      }
    }
  ]
}