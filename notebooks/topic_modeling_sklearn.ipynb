{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "sklearn.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-clematide/colab-notebooks-for-teaching/blob/main/notebooks/topic_modeling_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling Demo on English Newsgroup Texts\n",
        "  - Illustrates the use of term frequency (TF) and TF*IDF document representation\n",
        "  - Shows interactive topic modeling exploration with pyLDAvis"
      ],
      "metadata": {
        "id": "3pmkjbPyJy-k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Gbuq1DsIzC"
      },
      "source": [
        "# Setup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft-MR3lqsWCk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "b1fc255b-eb0d-46a7-bfe8-b4ec2699693b"
      },
      "source": [
        "%pip install gensim==4.3.3 numpy==1.26.4 pyldavis\n",
        "\n",
        "# colab has newer versions installed, we need to restart the runtime\n",
        "from IPython.display import HTML, display\n",
        "display(HTML(\"\"\"Please restart the runtime from the Menu Runtime if new packages were installed.<br><br>\n",
        "         <code>Runtime → Restart runtime</code><br><br>\n",
        "    This is necessary to apply the newly installed packages.\n",
        "    \"\"\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.3.3 in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pyldavis in /usr/local/lib/python3.12/dist-packages (3.4.1)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim==4.3.3) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim==4.3.3) (7.5.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyldavis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from pyldavis) (1.5.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from pyldavis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from pyldavis) (2.14.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.12/dist-packages (from pyldavis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pyldavis) (1.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from pyldavis) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyldavis) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyldavis) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyldavis) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim==4.3.3) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->pyldavis) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.17.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Please restart the runtime from the Menu Runtime if new packages were installed.<br><br>\n",
              "         <code>Runtime → Restart runtime</code><br><br>\n",
              "    This is necessary to apply the newly installed packages.\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try to avoid warnings but not really working for now\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"ipykernel.ipkernel\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"jupyter_client.*\") # Added specific filter"
      ],
      "metadata": {
        "id": "Ydh_3NUfucMv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1s9S9LEsIzG"
      },
      "source": [
        "# `pyLDAvis`\n",
        "\n",
        "pyLDAvis now also supports LDA application from scikit-learn. Let's take a look into this in more detail. We will be using the 20 newsgroups dataset as provided by scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-QCt0lSQsIzG"
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjcrWF1VsIzI"
      },
      "source": [
        "## Load 20 newsgroups dataset\n",
        "\n",
        "First, the 20 newsgroups dataset available in sklearn is loaded. As always, the headers, footers and quotes are removed.\n",
        "\n",
        "Newsgroup categories:\n",
        "`['alt.atheism',\n",
        " 'comp.graphics',\n",
        " 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware',\n",
        " 'comp.sys.mac.hardware',\n",
        " 'comp.windows.x',\n",
        " 'misc.forsale',\n",
        " 'rec.autos',\n",
        " 'rec.motorcycles',\n",
        " 'rec.sport.baseball',\n",
        " 'rec.sport.hockey',\n",
        " 'sci.crypt',\n",
        " 'sci.electronics',\n",
        " 'sci.med',\n",
        " 'sci.space',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        " 'talk.politics.mideast',\n",
        " 'talk.politics.misc',\n",
        " 'talk.religion.misc']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzlyi0f_sIzI"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHitBJqrsIzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "950dfa91-868f-46bd-d7b0-201042153727"
      },
      "source": [
        "cats = ['sci.med', 'alt.atheism', 'rec.autos', 'sci.space','rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),categories=cats)\n",
        "docs_raw = newsgroups.data\n",
        "print(len(docs_raw))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQHMVL4asIzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb98dfa3-2716-4892-c7fe-bf2df4a2e9e1"
      },
      "source": [
        "print(docs_raw[72])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, he's not nuts, WIP is second to none THE sports station.  They\n",
            "don't have Tony Bruno working ESPN radio and Al Morganti doing Friday\n",
            "Night Hockey because they suck.  I live in Richmond Va, but I visit\n",
            "Phila often, and on the way I get WTEM Washington) and WIP.  I hear\n",
            "the FAN at night wherever I go (the signal used to be WNBC, when they\n",
            "played golden oldies) because you can't avoid it.  Of those three,\n",
            "WIP has the best hosts hands down.  Chuck Cooperstein isn't a homer,\n",
            "and neither is Jody Mac.  WTEM is too generic to be placed in the\n",
            "catergory.  In fact if you have heard WTEM and the FAN you notice the\n",
            "theme music is identical...same ownership?? I think so!  WIP is\n",
            "totally original.  Their hosts actually have a personality (this is a\n",
            "knock at TEM (the TEAM) not the FAN because Mike and the Mad Dog and\n",
            "Sommers are good) I mean comparing the morning guys in Philadelphia\n",
            "to the ones in Washington is a total joke.  Anyway, I like the FAN\n",
            "and WIP, but I think the edge goes to 'IP.  \n",
            "\n",
            "When I get back from Philly, I go into withdraw cause Richmond has\n",
            "nada except the national sports line (and those guys are totally\n",
            "clueless)   \n",
            "I was really mad when WCAU was cancelled because they had Steve\n",
            "Fredericks doing sports phone after the Phillies games.  (WCAU is\n",
            "another strong station, now it's an oldies station, but they still\n",
            "have the Phillies) I started listening to the FAN because I heard he\n",
            "went there.  I finally heard him last summer and he wasn't the same\n",
            "guy.  Those NY fans got to him.  I was glad to hear him back in\n",
            "Philly when I went to see a few Eagles games.  \n",
            "\n",
            "\n",
            "I will admit, I am  die hard EAGLES fan and WIP is basically an\n",
            "Eagles station 365 days a year.  BUT, I bet you the Phillies are in\n",
            "control right now.\n",
            "\n",
            "\n",
            "About the knock on G. Cobb, I like him.  He knows the Eagles like a\n",
            "book.  I remember the weekend before they went to play San Fran,\n",
            "(when everyone thought the Eagles would be blown away) Cobb said that\n",
            "the Eagles usually play their best when no one believe they can win.\n",
            "Well they were inches shy of pulling the victory.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxtpHRQssIzL"
      },
      "source": [
        "## Convert to document-term matrix\n",
        "\n",
        "Next, the raw documents are converted into document-term matrix, possibly as raw counts or in TF-IDF form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC3UB1yKsIzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3294ebd8-e6da-460a-973e-65c96f904ad4"
      },
      "source": [
        "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
        "                                stop_words = 'english',\n",
        "                                lowercase = True,\n",
        "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
        "                                max_df = 0.5,  # exclude words with a relative document frequency greater than 50%\n",
        "                                min_df = 10    # exclude tokens that occur less than 10 times\n",
        "                                )\n",
        "dtm_tf = tf_vectorizer.fit_transform(docs_raw)\n",
        "print(dtm_tf.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2858, 3234)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How does a certain document look like in this representation?\n",
        "# Get the mapping of column indices to vocabulary items\n",
        "index2vocabulary_item = tf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get the dense matrix representation of the document-term matrix\n",
        "doc_index = 72  # Index of the document to show\n",
        "doc_matrix = dtm_tf.getrow(doc_index).toarray()\n",
        "\n",
        "# Print the words and their counts in the document\n",
        "for i, count in enumerate(doc_matrix[0]):\n",
        "    if count > 0:\n",
        "        word = index2vocabulary_item[i]\n",
        "        print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "jM8VvNjPvPpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a85ba4e-a65e-4701-c888-3f43f0d1fb43"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actually: 1\n",
            "admit: 1\n",
            "avoid: 1\n",
            "away: 1\n",
            "basically: 1\n",
            "believe: 1\n",
            "best: 2\n",
            "bet: 1\n",
            "blown: 1\n",
            "book: 1\n",
            "cause: 1\n",
            "chuck: 1\n",
            "cobb: 2\n",
            "comparing: 1\n",
            "control: 1\n",
            "days: 1\n",
            "die: 1\n",
            "doing: 2\n",
            "don: 1\n",
            "edge: 1\n",
            "fact: 1\n",
            "fan: 6\n",
            "fans: 1\n",
            "finally: 1\n",
            "friday: 1\n",
            "games: 2\n",
            "glad: 1\n",
            "goes: 1\n",
            "golden: 1\n",
            "good: 1\n",
            "got: 1\n",
            "guy: 1\n",
            "guys: 2\n",
            "hands: 1\n",
            "hard: 1\n",
            "hear: 2\n",
            "heard: 3\n",
            "homer: 1\n",
            "identical: 1\n",
            "inches: 1\n",
            "isn: 1\n",
            "joke: 1\n",
            "knock: 2\n",
            "knows: 1\n",
            "like: 3\n",
            "line: 1\n",
            "listening: 1\n",
            "live: 1\n",
            "mac: 1\n",
            "mad: 2\n",
            "mean: 1\n",
            "mike: 1\n",
            "morning: 1\n",
            "music: 1\n",
            "national: 1\n",
            "night: 2\n",
            "notice: 1\n",
            "nuts: 1\n",
            "ones: 1\n",
            "original: 1\n",
            "philadelphia: 1\n",
            "phillies: 3\n",
            "phone: 1\n",
            "placed: 1\n",
            "play: 2\n",
            "played: 1\n",
            "radio: 1\n",
            "really: 1\n",
            "remember: 1\n",
            "richmond: 2\n",
            "right: 1\n",
            "said: 1\n",
            "san: 1\n",
            "second: 1\n",
            "signal: 1\n",
            "sports: 3\n",
            "started: 1\n",
            "station: 4\n",
            "steve: 1\n",
            "strong: 1\n",
            "suck: 1\n",
            "summer: 1\n",
            "team: 1\n",
            "think: 2\n",
            "thought: 1\n",
            "tony: 1\n",
            "total: 1\n",
            "totally: 2\n",
            "used: 1\n",
            "usually: 1\n",
            "visit: 1\n",
            "washington: 2\n",
            "wasn: 1\n",
            "way: 1\n",
            "weekend: 1\n",
            "went: 3\n",
            "win: 1\n",
            "working: 1\n",
            "year: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative, we can build a tf-idf document-term matrix"
      ],
      "metadata": {
        "id": "8fFYC8CaFE6d"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikLmiOosIzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c97af82-80c4-4023-b0e5-71ec081b620c"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
        "dtm_tfidf = tfidf_vectorizer.fit_transform(docs_raw)\n",
        "print(dtm_tfidf.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2858, 3234)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How does a certain document look like in this representation?\n",
        "# Get the mapping of column indices to vocabulary items\n",
        "index2vocabulary_item = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get the dense matrix representation of the document-term matrix\n",
        "doc_index = 72  # Index of the document to show\n",
        "doc_matrix = dtm_tfidf.getrow(doc_index).toarray()\n",
        "\n",
        "# Print the words and their counts in the document\n",
        "for i, count in enumerate(doc_matrix[0]):\n",
        "    if count > 0:\n",
        "        word = index2vocabulary_item[i]\n",
        "        print(f\"{word:<12}: {count:.4f}\")"
      ],
      "metadata": {
        "id": "N3bwjGiaxvja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a8ba64-beea-43da-b1bb-930ab770d1e8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actually    : 0.0490\n",
            "admit       : 0.0730\n",
            "avoid       : 0.0655\n",
            "away        : 0.0546\n",
            "basically   : 0.0648\n",
            "believe     : 0.0468\n",
            "best        : 0.0995\n",
            "bet         : 0.0767\n",
            "blown       : 0.0827\n",
            "book        : 0.0584\n",
            "cause       : 0.0549\n",
            "chuck       : 0.0865\n",
            "cobb        : 0.1689\n",
            "comparing   : 0.0804\n",
            "control     : 0.0593\n",
            "days        : 0.0552\n",
            "die         : 0.0743\n",
            "doing       : 0.1079\n",
            "don         : 0.0335\n",
            "edge        : 0.0819\n",
            "fact        : 0.0507\n",
            "fan         : 0.4015\n",
            "fans        : 0.0701\n",
            "finally     : 0.0664\n",
            "friday      : 0.0790\n",
            "games       : 0.1180\n",
            "glad        : 0.0767\n",
            "goes        : 0.0602\n",
            "golden      : 0.0854\n",
            "good        : 0.0385\n",
            "got         : 0.0481\n",
            "guy         : 0.0622\n",
            "guys        : 0.1383\n",
            "hands       : 0.0747\n",
            "hard        : 0.0561\n",
            "hear        : 0.1259\n",
            "heard       : 0.1672\n",
            "homer       : 0.0854\n",
            "identical   : 0.0811\n",
            "inches      : 0.0827\n",
            "isn         : 0.0514\n",
            "joke        : 0.0835\n",
            "knock       : 0.1730\n",
            "knows       : 0.0677\n",
            "like        : 0.1004\n",
            "line        : 0.0589\n",
            "listening   : 0.0854\n",
            "live        : 0.0584\n",
            "mac         : 0.0767\n",
            "mad         : 0.1671\n",
            "mean        : 0.0523\n",
            "mike        : 0.0674\n",
            "morning     : 0.0743\n",
            "music       : 0.0845\n",
            "national    : 0.0603\n",
            "night       : 0.1255\n",
            "notice      : 0.0718\n",
            "nuts        : 0.0827\n",
            "ones        : 0.0610\n",
            "original    : 0.0626\n",
            "philadelphia: 0.0726\n",
            "phillies    : 0.2166\n",
            "phone       : 0.0701\n",
            "placed      : 0.0738\n",
            "play        : 0.1204\n",
            "played      : 0.0747\n",
            "radio       : 0.0669\n",
            "really      : 0.0438\n",
            "remember    : 0.0536\n",
            "richmond    : 0.1709\n",
            "right       : 0.0444\n",
            "said        : 0.0469\n",
            "san         : 0.0686\n",
            "second      : 0.0540\n",
            "signal      : 0.0752\n",
            "sports      : 0.2242\n",
            "started     : 0.0608\n",
            "station     : 0.2541\n",
            "steve       : 0.0677\n",
            "strong      : 0.0641\n",
            "suck        : 0.0835\n",
            "summer      : 0.0734\n",
            "team        : 0.0544\n",
            "think       : 0.0707\n",
            "thought     : 0.0519\n",
            "tony        : 0.0811\n",
            "total       : 0.0631\n",
            "totally     : 0.1437\n",
            "used        : 0.0461\n",
            "usually     : 0.0568\n",
            "visit       : 0.0773\n",
            "washington  : 0.1383\n",
            "wasn        : 0.0605\n",
            "way         : 0.0421\n",
            "weekend     : 0.0762\n",
            "went        : 0.1815\n",
            "win         : 0.0610\n",
            "working     : 0.0639\n",
            "year        : 0.0428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEiYoOK-sIzM"
      },
      "source": [
        "## Fit Latent Dirichlet Allocation models\n",
        "\n",
        "Finally, the LDA models are fitted. n_components is number of topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g7E6sE2sIzN"
      },
      "source": [
        "# for TF DTM\n",
        "lda_tf = LatentDirichletAllocation(n_components=10, random_state=0,verbose=1, max_iter=10)\n",
        "lda_tf.fit(dtm_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_8bg741sIzN"
      },
      "source": [
        "## Visualizing the models with pyLDAvis\n",
        "Multidimensional scaling = Dimension reduction\n",
        "\n",
        "Can you reidentify the newsgroups? `['sci.med', 'alt.atheism', 'rec.autos', 'sci.space','rec.sport.baseball']`\n",
        "\n",
        "Hover over topics circles and terms to explore the connection between words and topics..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "lGdYc2f0sIzN"
      },
      "source": [
        "pyLDAvis.lda_model.prepare(lda_tf, dtm_tf, tf_vectorizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topix Modeling with TFIDF values"
      ],
      "metadata": {
        "id": "8XOv67b8GGzg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuiVrI4usIzO"
      },
      "source": [
        "# for TFIDF DTM\n",
        "lda_tfidf = LatentDirichletAllocation(n_components=10, random_state=0, verbose=1,max_iter=10)\n",
        "lda_tfidf.fit(dtm_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ysSXJxLsIzO"
      },
      "source": [
        "pyLDAvis.lda_model.prepare(lda_tfidf, dtm_tfidf, tfidf_vectorizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Uvl72P0asIzO"
      },
      "source": [
        "### Using different MDS functions\n",
        "\n",
        "With `sklearn` installed, other MDS functions, such as MMDS and TSNE can be used for plotting if the default PCoA is not satisfactory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le2YQ_nJsIzO"
      },
      "source": [
        "pyLDAvis.lda_model.prepare(lda_tf, dtm_tf, tf_vectorizer, mds='mmds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWF9ZPKmsIzP"
      },
      "source": [
        "pyLDAvis.lda_model.prepare(lda_tf, dtm_tf, tf_vectorizer, mds='tsne')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "05tlg25CsIzP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}