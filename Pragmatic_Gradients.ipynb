{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPZomTwe+ABCnbcDopaJj2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-clematide/colab-notebooks-for-teaching/blob/main/Pragmatic_Gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pragmatic Gradients in Pytorch\n",
        "This notebook shows how practical machine learning \"monkey\" patches common mathematically undefined gradients"
      ],
      "metadata": {
        "id": "9PmXwCs-wUdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLu\n",
        "Not smooth at $x= 0$"
      ],
      "metadata": {
        "id": "Lv2_iKixw65c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a tensor with a value of 0\n",
        "x = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "# Apply ReLU\n",
        "relu_output = torch.relu(x)\n",
        "\n",
        "# Perform backpropagation\n",
        "relu_output.backward()\n",
        "\n",
        "# Output the gradient\n",
        "print(\"ReLU Output:\", relu_output)  # Should output: tensor(0.)\n",
        "print(\"Gradient at x=0:\", x.grad)    # Should output: tensor(0.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBxii4lbxNiV",
        "outputId": "e69b26a2-1237-413b-ace2-cd2ae3c63aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU Output: tensor(0., grad_fn=<ReluBackward0>)\n",
            "Gradient at x=0: tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## maximum function"
      ],
      "metadata": {
        "id": "iaw1bcC5khzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a tensor with zero\n",
        "x = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "# Compute the absolute value\n",
        "abs_value = torch.abs(x)\n",
        "\n",
        "# Backpropagation\n",
        "abs_value.backward()\n",
        "\n",
        "# Output the results\n",
        "print(\"Absolute Value:\", abs_value)          # Outputs: tensor(0.)\n",
        "print(\"Gradient:\", x.grad)                    # Outputs: tensor(0.)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbKYbFNpKD9Y",
        "outputId": "28a48318-d1a8-4074-e294-54397000e80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolute Value: tensor(0., grad_fn=<AbsBackward0>)\n",
            "Gradient: tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPwuBDppIqXJ",
        "outputId": "ef8ac723-609f-4b78-845e-e86825d17075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5000)\n",
            "tensor(0.5000)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define two equal tensors\n",
        "a = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# Perform max operation\n",
        "max_value = torch.max(a, b)\n",
        "\n",
        "# Backpropagation\n",
        "max_value.backward()\n",
        "\n",
        "# Gradients\n",
        "print(a.grad)  # Output: tensor(1.)\n",
        "print(b.grad)  # Output: tensor(0.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## softmax function"
      ],
      "metadata": {
        "id": "sw2bLk7Hkrc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define equal logits\n",
        "logits = torch.tensor([1.0, 1.0], requires_grad=True)\n",
        "\n",
        "# Apply softmax\n",
        "probabilities = torch.softmax(logits, dim=0)\n",
        "\n",
        "# Define a target for cross-entropy loss\n",
        "target = torch.tensor([1])  # Assuming we want the first class\n",
        "\n",
        "# Compute cross-entropy loss\n",
        "loss = nn.CrossEntropyLoss()(logits.unsqueeze(0), target)\n",
        "\n",
        "# Backpropagation\n",
        "loss.backward()\n",
        "\n",
        "print(\"Softmax Output:\", probabilities)  # Should show equal probabilities\n",
        "print(\"Gradients:\", logits.grad)          # Should not be zero\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqtUOFDIJxnI",
        "outputId": "890c03cf-c66d-408c-fca8-b0a8db06a02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Output: tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
            "Gradients: tensor([ 0.5000, -0.5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy\n"
      ],
      "metadata": {
        "id": "lOCkcnx7Ktyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import *\n"
      ],
      "metadata": {
        "id": "uphZ_yoIN07m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = Symbol('y',real=True) # Define y as a symbol\n",
        "diff(abs(y),y) # for"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "oXFnmxLkN3yb",
        "outputId": "e86d1c45-be55-4dfc-d545-f3e42c3a8735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sign(y)"
            ],
            "text/latex": "$\\displaystyle \\operatorname{sign}{\\left(y \\right)}$"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff(sign(y),y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "r4fWbEEVN6vI",
        "outputId": "99d18b17-e5e3-403d-9c47-11012383a6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2*DiracDelta(y)"
            ],
            "text/latex": "$\\displaystyle 2 \\delta\\left(y\\right)$"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([0.0, 2.0, -3.0], requires_grad=True)\n",
        "y = torch.sign(x)\n",
        "z = y.sum()\n",
        "z.backward()\n",
        "\n",
        "print(x.grad) # Output: tensor([0., 0., 0.])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnn0mY1OO45F",
        "outputId": "122a50ff-16b9-48be-97af-9f6dec69c90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Input tensor\n",
        "x = torch.tensor([0.0, 2.0, -3.0], requires_grad=True)\n",
        "\n",
        "# Compute the sign\n",
        "y = torch.sign(x)\n",
        "\n",
        "# Sum the signs\n",
        "z = y.sum()\n",
        "\n",
        "# Backpropagation\n",
        "z.backward()\n",
        "\n",
        "# Output the gradients\n",
        "print(x.grad)  # Output: tensor([0., 0., 0.])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CfA3aMzPVGt",
        "outputId": "4d2f6823-25db-47c6-e9a0-548fb9aa86b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLabwgbgPoW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "chQA3iCUwTZS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dx3MaD6nwaMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}