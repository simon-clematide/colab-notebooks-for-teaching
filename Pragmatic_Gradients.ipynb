{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTt2DKK+nx0h5Kzja5boVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-clematide/colab-notebooks-for-teaching/blob/main/Pragmatic_Gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pragmatic Gradients in Pytorch\n",
        "This notebook shows how practical machine learning \"monkey\" patches common mathematically undefined gradients"
      ],
      "metadata": {
        "id": "9PmXwCs-wUdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLu\n",
        "Not smooth at $x= 0$"
      ],
      "metadata": {
        "id": "Lv2_iKixw65c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a tensor with a value of 0\n",
        "x = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "# Apply ReLU\n",
        "relu_output = torch.relu(x)\n",
        "\n",
        "# Perform backpropagation\n",
        "relu_output.backward()\n",
        "\n",
        "# Output the gradient\n",
        "print(\"ReLU Output:\", relu_output)  # Should output: tensor(0.)\n",
        "print(\"Gradient at x=0:\", x.grad)    # Should output: tensor(0.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBxii4lbxNiV",
        "outputId": "e69b26a2-1237-413b-ace2-cd2ae3c63aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU Output: tensor(0., grad_fn=<ReluBackward0>)\n",
            "Gradient at x=0: tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## maximum function"
      ],
      "metadata": {
        "id": "iaw1bcC5khzh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPwuBDppIqXJ",
        "outputId": "ef8ac723-609f-4b78-845e-e86825d17075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5000)\n",
            "tensor(0.5000)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define two equal tensors\n",
        "a = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# Perform max operation\n",
        "max_value = torch.max(a, b)\n",
        "\n",
        "# Backpropagation\n",
        "max_value.backward()\n",
        "\n",
        "# Gradients\n",
        "print(a.grad)  # Output: tensor(1.)\n",
        "print(b.grad)  # Output: tensor(0.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## abs function"
      ],
      "metadata": {
        "id": "ULUc9nGkosY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a tensor with zero\n",
        "x = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "# Compute the absolute value\n",
        "abs_value = torch.abs(x)\n",
        "\n",
        "# Backpropagation\n",
        "abs_value.backward()\n",
        "\n",
        "# Output the results\n",
        "print(\"Absolute Value:\", abs_value)          # Outputs: tensor(0.)\n",
        "print(\"Gradient:\", x.grad)                    # Outputs: tensor(0.)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbKYbFNpKD9Y",
        "outputId": "3df3ebb6-3011-4927-ae0f-61efc83019ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolute Value: tensor(0., grad_fn=<AbsBackward0>)\n",
            "Gradient: tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## softmax function"
      ],
      "metadata": {
        "id": "sw2bLk7Hkrc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define equal logits\n",
        "logits = torch.tensor([1.0, 1.0], requires_grad=True)\n",
        "\n",
        "# Apply softmax\n",
        "probabilities = torch.softmax(logits, dim=0)\n",
        "\n",
        "# Define a target for cross-entropy loss\n",
        "target = torch.tensor([1])  # Assuming we want the first class\n",
        "\n",
        "# Compute cross-entropy loss\n",
        "loss = nn.CrossEntropyLoss()(logits.unsqueeze(0), target)\n",
        "\n",
        "# Backpropagation\n",
        "loss.backward()\n",
        "\n",
        "print(\"Softmax Output:\", probabilities)  # Should show equal probabilities\n",
        "print(\"Gradients:\", logits.grad)          # Should not be zero\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqtUOFDIJxnI",
        "outputId": "890c03cf-c66d-408c-fca8-b0a8db06a02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Output: tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
            "Gradients: tensor([ 0.5000, -0.5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dot product"
      ],
      "metadata": {
        "id": "chQA3iCUwTZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "b = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "dot_product = torch.dot(a, b)\n",
        "dot_product.backward()\n",
        "\n",
        "# Gradients\n",
        "print(a.grad)  # Should give tensor([3.0, 4.0])\n",
        "print(b.grad)  # Should give tensor([1.0, 2.0])"
      ],
      "metadata": {
        "id": "dx3MaD6nwaMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6accdecd-5c66-491d-c9b3-2b436aebaae5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 4.])\n",
            "tensor([1., 2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix-Vector Multiplication"
      ],
      "metadata": {
        "id": "IZxeoixivfGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "x = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "z = torch.matmul(A, x)\n",
        "\n",
        "z.backward(torch.ones_like(z))\n",
        "print(x.grad)  # Gradients with respect to x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r0f-i-3viFY",
        "outputId": "b7ecd4d5-f8a4-441c-d5e6-75118b033d6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No gradient"
      ],
      "metadata": {
        "id": "mmW_g6p_uoYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.,requires_grad=False)\n",
        "y = torch.tensor(4.,requires_grad=True)\n",
        "f=(x*x)*y+(y+2)"
      ],
      "metadata": {
        "id": "kHdmLo88o3JK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiMLn2gWu4SA",
        "outputId": "548e00e2-dae2-4274-ab62-3b3e00cb8bcb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(42., grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f.backward()"
      ],
      "metadata": {
        "id": "GVe7fIZeu7xI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad, y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga_jjJVTu_wW",
        "outputId": "9208f3b3-7bf0-4fce-ce01-c7ef597cecf4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, tensor(10.))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OBZOdsXovDG0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}